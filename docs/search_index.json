[["index.html", "Desagregación de datos en encuestas de hogares: metodologías bayesianas para modelos de estimación en áreas pequeñas Agenda", " Desagregación de datos en encuestas de hogares: metodologías bayesianas para modelos de estimación en áreas pequeñas Andrés Gutiérrez1, Stalyn Guerrero2 2023-03-31 Agenda Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["material-del-curso.html", "Material del curso", " Material del curso En el siguiente enlace encontrará material bibliográfico complementario (Libros, presentaciones, casos de estudio y manuales de instalación) Descargar En el siguiente enlace encontrará las rutinas de R desarrolladas para el taller. Descargar "],["día-1---sesión-1--no-dejar-a-nadie-atrás---ods-y-la-agenda-2030.html", "Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030", " Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030 Ver presentación "],["día-1---sesión-2--censo-e-información-satelital.html", "Capítulo 2 Día 1 - Sesión 2- Censo e información satelital ", " Capítulo 2 Día 1 - Sesión 2- Censo e información satelital "],["uso-de-imágenes-satelitales-y-sae.html", "2.1 Uso de imágenes satelitales y SAE", " 2.1 Uso de imágenes satelitales y SAE Uno de los artículo pioneros de estimación de áreas pequeñas fue el artículo de Singh, R, et. al. (2002) el cual abordó la estimación del rendimiento de cultivos para los tehsil (unidad subadministrativa) del distriyo Rohtak district en Haryana (India). Las imágenes raster representan el mundo mediante un conjunto de celdas contiguas igualmente espaciadas conocidas como pixeles, estas imágenes tienen información como un sistema de información geográfico, Un sistema de referencia de coordenadas. Las imágenes almacenan un identificador, un valor en cada pixel (o un vector con diferentes valores) y cada celda tiene asociada una escala de colores. Las imágenes pueden obtenerse crudas y procesadas, estas primeras contienen solamente las capas de colores, las segundas contienen también valores que han sido procesados en cada celda (índices de vegetación, intensidad lumínica, tipo de vegetación). La información cruda puede utilizarse para entrenar características que se desean entrenar (carreteras, tipo de cultivo, bosque / no bosque), afortunadamente en Google Earth Engine encontramos muchos indicadores procesadas asociadas a un pixel. Estos indicadores pueden agregarse a nivel de un área geográfica. 2.1.1 Fuentes de datos de imágenes satelitales Algunas de las principales fuentes de imágenes satelitales son: http://earthexplorer.usgs.gov/ https://lpdaacsvc.cr.usgs.gov/appeears/ https://search.earthdata.nasa.gov/search https://scihub.coGTMnicus.eu/ https://aws.amazon.com/public-data-sets/landsat/ Sin embargo la mayor parte de estas fuentes están centralizadas en Google Earth Engine que permite buscar fuentes de datos provenientes de imágenes satelitales. GEE se puede manejar por medio de APIS en diferentes lenguajes de programación: Javascript (por defecto), Python y R (paquete rgee). "],["google-earth-eninge.html", "2.2 Google Earth Eninge", " 2.2 Google Earth Eninge Crear una cuenta en link, una vez que se ingrese a la cuenta puede buscarse los conjuntos de datos de interés: Una vez se busque el conjunto de datos se puede abrir un editor de código brindado por google en Javascript. Copiar y pegar la sintaxis que brinda el buscador de conjunto de datos para visualizar la imagen raster y disponer de sentencias que GTMmitan la obtención del conjunto de datos de interés posteriormente en R "],["instalación-de-rgee.html", "2.3 Instalación de rgee", " 2.3 Instalación de rgee Descargar e instalar anaconda o conda. (https://www.anaconda.com/products/individual) Abrir Anaconda prompt y configurar ambiente de trabajo (ambiente python rgee_py) con las siguientes sentencias: conda create -n rgee_py python=3.9 activate rgee_py pip install google-api-python-client pip install earthengine-api pip install numpy Listar los ambientes de Python disponibles en anaconda prompt conda env list Una vez identificado la ruta del ambiente ambiente rgee_py definirla en R (no se debe olvidar cambiar \\ por /). Instalar reticulate y rgee, cargar paquetes para procesamiento espacial y configurar el ambiente de trabajo como sigue: library(reticulate) # Conexión con Python library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(dplyr) # Paquete para procesamiento de datos rgee_environment_dir = &quot;C://Users//sguerrero//Anaconda3//envs//rgee_py//python.exe&quot; # Configurar python (Algunas veces no es detectado y se debe reiniciar R) reticulate::use_python(rgee_environment_dir, required=T) rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) Una vez configurado el ambiente puede iniciarlizarse una sesión de Google Earth Engine como sigue: rgee::ee_Initialize(drive = T) Notas: Se debe inicializar cada sesión con el comando rgee::ee_Initialize(drive = T). Los comandos de javascript que invoquen métodos con “.” se sustituyen por signo peso ($), por ejemplo: ee.ImageCollection().filterDate() # Javascript ee$ImageCollection()$filterDate() # R 2.3.1 Descargar información satelital Paso 1: disponer de los shapefile # shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/CHL_dam2.shp&quot;) shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/CHL_dam.shp&quot;) plot(shape[&quot;geometry&quot;]) Paso 2: Seleccionar el archivo de imágenes que desea procesar, para nuestro ejemplo luces nocturnas. luces &lt;- ee$ImageCollection(&quot;NOAA/DMSP-OLS/NIGHTTIME_LIGHTS&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2013-01-01&quot;, &quot;2014-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;stable_lights&quot;)) %&gt;% ee$ImageCollection$toBands() Paso 3: Descargar la información ## Tiempo 10 minutos shape_luces &lt;- map(unique(shape$dam), ~tryCatch(ee_extract( x = luces, y = shape[&quot;dam&quot;] %&gt;% filter(dam == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(dam = .x), error = function(e)data.frame(dam = .x))) shape_luces %&lt;&gt;% bind_rows() tba(shape_luces, cap = &quot;Promedio de luces nocturnasa&quot;) Tabla 2.1: Promedio de luces nocturnasa dam luces_nocturnas 05 0.2383 12 0.0731 11 0.0348 10 0.6748 15 0.4691 01 0.5006 02 0.6402 03 0.4240 04 1.2168 13 10.6203 14 0.9825 09 1.4667 08 2.6897 16 1.9556 07 1.8539 06 3.5849 Repetir la rutina para: Tipo de suelo: crops-coverfraction (Porcentaje de cubrimiento cultivos) y urban-coverfraction (Porcentaje de cobertura urbana) disponibles en https://develoGTMs.google.com/earth-engine/datasets/catalog/COGTMNICUS_Landcover_100m_Proba-V-C3_Global#description Tiempo de viaje al hospital o clínica más cercana (accessibility) y tiempo de viaje al hospital o clínica más cercana utilizando transporte no motorizado (accessibility_walking_only) información disponible en https://develoGTMs.google.com/earth-engine/datasets/catalog/Oxford_MAP_accessibility_to_healthcare_2019 Modificación humana, donde se consideran los asentamiento humano, la agricultura, el transporte, la minería y producción de energía e infraestructura eléctrica. En el siguiente link encuentra la información satelital https://develoGTMs.google.com/earth-engine/datasets/catalog/CSP_HM_GlobalHumanModification#description Paso 4 consolidar la información. dam luces_nocturnas cubrimiento_cultivo cubrimiento_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado 05 0.2383 7.5529 2.1488 0.3131 101.1488 481.0220 12 0.0731 0.0014 0.0486 0.0419 557.4796 2410.1264 11 0.0348 0.4897 0.0455 0.0446 472.6701 1706.0858 10 0.6748 6.2825 0.4273 0.1569 182.5853 530.2074 15 0.4691 0.3049 0.3165 0.0790 129.5948 634.6007 01 0.5006 0.0630 0.1962 0.0576 149.3803 706.6626 02 0.6402 0.0106 0.1421 0.0655 239.0077 993.2293 03 0.4240 0.0887 0.2277 0.0702 261.4221 1033.4120 04 1.2168 1.2978 0.4280 0.1556 157.2359 558.3460 13 10.6203 11.2553 5.3412 0.3404 108.0762 288.9854 Los resultados se muestran en los siguientes mapas 2.3.2 Luces nocturnas 2.3.3 Cubrimiento cultivos 2.3.4 Cubrimiento urbanos 2.3.5 Modificación humana 2.3.6 Tiempo promedio al hospital 2.3.7 Tiempo promedio al hospital en vehiculo no motorizado "],["censos-de-población-y-vivienda.html", "2.4 Censos de población y vivienda", " 2.4 Censos de población y vivienda Es necesario definir las variables del país con los que se desea trabajar. De acuerdo a esto, como primer paso se debe tener acceso al censo del país, para ello puede acceder desde el siguiente enlace https://redatam.org/en/microdata en el cual dispondrá de un archivo .zip con los microdatos del país. Ahora bien, para leer el conjunto de datos, es necesario emplear la función redatam.open de la librería redatam, la cual depende directamente del diccionario censal del software REDATAM, este es un archivo con extensión dicx y que debe encontrarse en la carpeta sobre los datos que se están leyendo. Así, es como se crea un objeto dentro de R que hace la respectiva unión del diccionario con los microdatos de la base de datos censal. La siguiente sintaxis muestra la lectura del diccionario en R y los cálculos iniciales library(redatam) chile &lt;- redatam.open(&quot;CHL/2017/1.Ingreso/Data/cpv2017chl-cde.dicx&quot;) CONTEOS &lt;- redatam.query( CHL, &quot;freq REGION.IDREGION by AREAUR.URBRUR by PERSONA.P09 by PERSONA.ANEST by PERSONA.P08 by PERSONA.PBLOPER&quot;, tot.omit = FALSE ) # Eliminando totales de la tabla CONTEOS2 &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(. != &quot;__tot__&quot;)) Después de realizar algunas validaciones se estandarizan las variables como muestra el siguiente código. censo_mrp &lt;- CONTEOS2 %&gt;% transmute( dam = str_pad( string = IDREGION1_value, width = 2, pad = &quot;0&quot; ), area = case_when(URBRUR2_value == 1 ~ &quot;1&quot;, # 1 = Urbana TRUE ~ &quot;0&quot;), # 0 = Rural sexo = as.character(P085_value), edad = case_when( P093_value %in% 0:14 ~ &quot;1&quot;, # 5 a 14 P093_value %in% 15:29 ~ &quot;2&quot;, # 15 a 29 P093_value %in% 30:44 ~ &quot;3&quot;, # 30 a 44 P093_value %in% 45:64 ~ &quot;4&quot;, # 45 a 64 TRUE ~ &quot;5&quot; ), # 65 o mas anoest = case_when( P093_value &lt; 6~ &quot;98&quot;, #No aplica ANEST4_value == 99 ~ &quot;99&quot;, #NS/NR ANEST4_value == 0 ~ &quot;1&quot;, # Sin educacion ANEST4_value %in% c(1:6) ~ &quot;2&quot;, # 1 - 6 ANEST4_value %in% c(7:12) ~ &quot;3&quot;, # 7 - 12 ANEST4_value &gt; 12 ~ &quot;4&quot;, # mas de 12 TRUE ~ &quot;Error&quot; ), etnia = case_when( PBLOPER6_value == 1 ~ &quot;1&quot;, # Indigenas PBLOPER6_value == 2 ~ &quot;2&quot;, # Afro TRUE ~ &quot;3&quot; # Otro ), value ) %&gt;% group_by(dam, area, etnia, sexo, edad, anoest) %&gt;% summarise(n = sum(value)) A partir de la base estandarizada es posible construir algunas covariables para el departamento. censo_mrp &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/censo_mrp_dam.rds&quot;) tasa_censo &lt;- model.matrix(dam ~ -1 +., data = censo_mrp %&gt;% select(-n)) %&gt;% data.frame() %&gt;% mutate(dam = censo_mrp$dam, n = censo_mrp$n) %&gt;% group_by(dam) %&gt;% summarise_all(~weighted.mean(x = .,w = n)) %&gt;% mutate(etnia1 = 1-etnia3-etnia2 ) %&gt;% select(-area0, -anoest98,-etnia3,-n) tba(tasa_censo) dam area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 anoest99 etnia1 01 0.9380 0.0014 0.4924 0.2482 0.2309 0.2118 0.0755 0.1555 0.4839 0.2053 0.0294 0.2409 02 0.9411 0.0009 0.4815 0.2504 0.2418 0.2226 0.0748 0.1379 0.4793 0.2336 0.0364 0.1348 03 0.9104 0.0005 0.4953 0.2289 0.2075 0.2361 0.0982 0.1710 0.4933 0.1879 0.0183 0.1932 04 0.8119 0.0003 0.5132 0.2274 0.2003 0.2372 0.1182 0.1834 0.4781 0.1850 0.0279 0.0855 05 0.9101 0.0002 0.5153 0.2313 0.1970 0.2450 0.1361 0.1599 0.4705 0.2342 0.0293 0.0657 06 0.7439 0.0001 0.5039 0.2104 0.2083 0.2547 0.1191 0.2114 0.4856 0.1611 0.0235 0.0626 07 0.7322 0.0001 0.5104 0.2178 0.2012 0.2539 0.1230 0.2184 0.4842 0.1501 0.0246 0.0468 08 0.8858 0.0001 0.5178 0.2326 0.1978 0.2493 0.1176 0.1869 0.4591 0.2174 0.0210 0.1074 09 0.7089 0.0001 0.5141 0.2254 0.1973 0.2427 0.1258 0.2158 0.4653 0.1678 0.0282 0.3356 10 0.7361 0.0001 0.5060 0.2202 0.2145 0.2454 0.1121 0.2228 0.4724 0.1651 0.0263 0.2760 11 0.7958 0.0001 0.4800 0.2131 0.2336 0.2383 0.0901 0.1990 0.4715 0.1820 0.0208 0.2817 12 0.9190 0.0001 0.4881 0.2217 0.2277 0.2485 0.1164 0.1578 0.4841 0.2409 0.0201 0.2268 13 0.9630 0.0004 0.5132 0.2434 0.2186 0.2364 0.1079 0.1480 0.4468 0.2692 0.0281 0.0973 14 0.7166 0.0001 0.5093 0.2297 0.1934 0.2508 0.1258 0.2068 0.4695 0.1756 0.0319 0.2502 15 0.9167 0.0205 0.5020 0.2425 0.2098 0.2213 0.1087 0.1555 0.4840 0.2088 0.0323 0.3284 16 0.6943 0.0001 0.5161 0.2096 0.1947 0.2648 0.1355 0.2267 0.4748 0.1603 0.0236 0.0464 El indicador es posible definirlo a partir de una variable del censo, haciendo que el proceso seá hace más corto, para este caso es empleada la variable VIVIENDA.P03C, agregada por dam En el primer bloque que código usando la función redatam.query() se realiza el conteo de viviendas por el material del piso. Seguido de esto, eliminamos los registros que no son de interés, por ejemplo, el total en el dam o total nacional, los cuales se identifican dentro de la base con la etiqueta __tot__. El siguiente paso es contar el número de viviendas por dam que tienen los pisos de tierra en el censo (Pobx) y el total de viviendas que respondieron a la pregunta (PobT), para finalmente realizar el cociente de estas dos preguntas. CONTEOS &lt;- redatam.query(chile, &quot;freq REGION.IDREGION by VIVIENDA.P03C&quot;, tot.omit = FALSE) PISO &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(!. %in% c(&quot;__tot__&quot;,&quot;__mv__&quot;) )) tasa_piso &lt;- PISO %&gt;% mutate(Pobx = ifelse(P03C2_value %in% c(5), value, 0), PobT = value) %&gt;% group_by( depto = IDREGION1_value ) %&gt;% summarise(PobT = sum(PobT), Pobx = sum(Pobx)) %&gt;% transmute(depto, piso_tierra = Pobx/PobT) La tabla resultante se muestra a continuación. dam piso_tierra 05 0.0016 12 0.0005 11 0.0012 10 0.0005 15 0.0222 01 0.0179 02 0.0092 03 0.0132 04 0.0110 13 0.0009 14 0.0006 09 0.0029 08 0.0018 16 0.0047 07 0.0061 06 0.0038 El proceso se repite con otras preguntas del censo hasta consolidar la tabla siguiente. predictors_censo_dam &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/predictors_censo_dam.rds&quot;) tba(predictors_censo_dam) dam area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 anoest99 etnia1 piso_tierra material_paredes material_techo rezago_escolar alfabeta tasa_desocupacion 05 0.9101 0.0002 0.5153 0.2313 0.1970 0.2450 0.1361 0.1599 0.4705 0.2342 0.0293 0.0657 0.0016 0.3236 0.7653 0.3365 0.0479 0.0683 12 0.9190 0.0001 0.4881 0.2217 0.2277 0.2485 0.1164 0.1578 0.4841 0.2409 0.0201 0.2268 0.0005 0.2649 0.8785 0.3382 0.0431 0.0402 11 0.7958 0.0001 0.4800 0.2131 0.2336 0.2383 0.0901 0.1990 0.4715 0.1820 0.0208 0.2817 0.0012 0.3167 0.9398 0.2793 0.0795 0.0445 10 0.7361 0.0001 0.5060 0.2202 0.2145 0.2454 0.1121 0.2228 0.4724 0.1651 0.0263 0.2760 0.0005 0.3166 0.9082 0.2491 0.0823 0.0612 15 0.9167 0.0205 0.5020 0.2425 0.2098 0.2213 0.1087 0.1555 0.4840 0.2088 0.0323 0.3284 0.0222 0.2416 0.5142 0.3194 0.0485 0.0743 01 0.9380 0.0014 0.4924 0.2482 0.2309 0.2118 0.0755 0.1555 0.4839 0.2053 0.0294 0.2409 0.0179 0.2913 0.5745 0.3199 0.0394 0.0728 02 0.9411 0.0009 0.4815 0.2504 0.2418 0.2226 0.0748 0.1379 0.4793 0.2336 0.0364 0.1348 0.0092 0.2216 0.5673 0.3545 0.0342 0.0776 03 0.9104 0.0005 0.4953 0.2289 0.2075 0.2361 0.0982 0.1710 0.4933 0.1879 0.0183 0.1932 0.0132 0.3880 0.8275 0.2802 0.0561 0.0957 04 0.8119 0.0003 0.5132 0.2274 0.2003 0.2372 0.1182 0.1834 0.4781 0.1850 0.0279 0.0855 0.0110 0.3256 0.7896 0.2796 0.0624 0.0848 13 0.9630 0.0004 0.5132 0.2434 0.2186 0.2364 0.1079 0.1480 0.4468 0.2692 0.0281 0.0973 0.0009 0.1369 0.5678 0.3851 0.0429 0.0661 14 0.7166 0.0001 0.5093 0.2297 0.1934 0.2508 0.1258 0.2068 0.4695 0.1756 0.0319 0.2502 0.0006 0.3159 0.9292 0.2643 0.0866 0.0688 09 0.7089 0.0001 0.5141 0.2254 0.1973 0.2427 0.1258 0.2158 0.4653 0.1678 0.0282 0.3356 0.0029 0.3044 0.9193 0.2526 0.0896 0.0740 08 0.8858 0.0001 0.5178 0.2326 0.1978 0.2493 0.1176 0.1869 0.4591 0.2174 0.0210 0.1074 0.0018 0.2106 0.8356 0.3099 0.0689 0.0857 16 0.6943 0.0001 0.5161 0.2096 0.1947 0.2648 0.1355 0.2267 0.4748 0.1603 0.0236 0.0464 0.0047 0.2727 0.8988 0.2332 0.0936 0.0829 07 0.7322 0.0001 0.5104 0.2178 0.2012 0.2539 0.1230 0.2184 0.4842 0.1501 0.0246 0.0468 0.0061 0.2498 0.8574 0.2235 0.0917 0.0680 06 0.7439 0.0001 0.5039 0.2104 0.2083 0.2547 0.1191 0.2114 0.4856 0.1611 0.0235 0.0626 0.0038 0.2575 0.8052 0.2401 0.0763 0.0670 2.4.1 Mapas de las variables con información censal. temp2 &lt;- predictors_censo_dam %&gt;% select(-dam) %&gt;% names() temp2 &lt;- paste0(&quot;Recursos/Día1/Sesion2/0Recursos/&quot;, temp2, &quot; .png&quot;) knitr::include_graphics(temp2) "],["día-1---sesión-3--fundamentos-de-la-inferencia-bayesiana-en-r-y-stan.html", "Capítulo 3 Día 1 - Sesión 3- Fundamentos de la inferencia Bayesiana en R y STAN", " Capítulo 3 Día 1 - Sesión 3- Fundamentos de la inferencia Bayesiana en R y STAN El proyecto Manhattan y la estimación desagregada con encuestas de hogares "],["día-1---sesión-4--modelos-sintéticos-simples.html", "Capítulo 4 Día 1 - Sesión 4- Modelos sintéticos simples ", " Capítulo 4 Día 1 - Sesión 4- Modelos sintéticos simples "],["regla-de-bayes.html", "4.1 Regla de Bayes", " 4.1 Regla de Bayes En términos de inferencia para \\(\\boldsymbol{\\theta}\\), es necesario encontrar la distribución de los parámetros condicionada a la observación de los datos. Para este fin, es necesario definir la distribución conjunta de la variable de interés con el vector de parámetros. \\[ p(\\boldsymbol{\\theta},\\mathbf{Y})=p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta}) \\] La distribución \\(p(\\boldsymbol{\\theta})\\) se le conoce con el nombre de distribución previa. El término \\(p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})\\) es la distribución de muestreo, verosimilitud o distribución de los datos. La distribución del vector de parámetros condicionada a los datos observados está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})=\\frac{p(\\boldsymbol{\\theta},\\mathbf{Y})}{p(\\mathbf{Y})}=\\frac{p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{Y})} \\] A la distribución \\(p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\) se le conoce con el nombre de distribución posterior. Nótese que el denominador no depende del vector de parámetros y considerando a los datos observados como fijos, corresponde a una constante y puede ser obviada. Por lo tanto, otra representación de la regla de Bayes está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\propto p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) \\] "],["inferencia-bayesiana..html", "4.2 Inferencia Bayesiana.", " 4.2 Inferencia Bayesiana. En términos de estimación, inferencia y predicción, el enfoque Bayesiano supone dos momentos o etapas: Antes de la recolección de las datos, en donde el investigador propone, basado en su conocimiento, experiencia o fuentes externas, una distribución de probabilidad previa para el parámetro de interés. Después de la recolección de los datos. Siguiendo el teorema de Bayes, el investigador actualiza su conocimiento acerca del comportamiento probabilístico del parámetro de interés mediante la distribución posterior de este. "],["modelos-uniparamétricos.html", "4.3 Modelos uniparamétricos", " 4.3 Modelos uniparamétricos Los modelos que están definidos en términos de un solo parámetro que pertenece al conjunto de los números reales se definen como modelos uniparamétricos. 4.3.1 Modelo de unidad: Bernoulli Suponga que \\(Y\\) es una variable aleatoria con distribución Bernoulli dada por: \\[ p(Y \\mid \\theta)=\\theta^y(1-\\theta)^{1-y}I_{\\{0,1\\}}(y) \\] Como el parámetro \\(\\theta\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible formular varias opciones para la distribución previa del parámetro. En particular, la distribución uniforme restringida al intervalo \\([0,1]\\) o la distribución Beta parecen ser buenas opciones. Puesto que la distribución uniforme es un caso particular de la distribución Beta. Por lo tanto la distribución previa del parámetro \\(\\theta\\) estará dada por \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] y la distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid Y \\sim Beta(y+\\alpha,\\beta-y+1) \\end{equation*} \\] Cuando se tiene una muestra aleatoria \\(Y_1,\\ldots,Y_n\\) de variables con distribución Bernoulli de parámetro \\(\\theta\\), entonces la distribución posterior del parámetro de interés es \\[ \\begin{equation*} \\theta \\mid Y_1,\\ldots,Y_n \\sim Beta\\left(\\sum_{i=1}^ny_i+\\alpha,\\beta-\\sum_{i=1}^ny_i+n\\right) \\end{equation*} \\] Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[ P_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario. Note que, \\[ \\begin{equation*} \\bar{Y}_d = P_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(P\\) esta dado por: \\[ \\hat{P} = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{P} = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 4.3.1.1 Práctica en R library(tidyverse) encuesta &lt;- readRDS(&quot;Recursos/Día1/Sesion4/Data/encuesta2017CHL.Rds&quot;) Sea \\(Y\\) la variable aleatoria \\[ Y_{i}=\\begin{cases} 1 &amp; ingreso&lt;lp\\\\ 0 &amp; ingreso\\geq lp \\end{cases} \\] El tamaño de la muestra es de 26434 Indígena datay &lt;- encuesta %&gt;% filter(etnia_ee == 1) %&gt;% transmute(y = ifelse(ingcorte &lt; lp, 1,0)) addmargins(table(datay$y)) 0 1 Sum 22448 3986 26434 Un grupo de estadístico experto decide utilizar una distribución previa Beta, definiendo los parámetros de la distribución previa como \\(Beta(\\alpha=1, \\beta=1)\\). La distribución posterior del parámetro de interés, que representa la probabilidad de estar por debajo de la linea de pobreza, es \\(Beta(3986 + 1, 1 - 3986 + 26434)=Beta(3987, 2.2449\\times 10^{4})\\) Figura 4.1: Distribución previa (línea roja) y distribución posterior (línea negra) La estimación del parámetro estaría dado por: \\[ E(X) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{3987}{3987+ 2.2449\\times 10^{4}} = 0.1508171 \\] luego, el intervalo de credibilidad para la distribución posterior es. n = length(datay$y) n1 = sum(datay$y) qbeta(c(0.025, 0.975), shape1 = 1 + n1, shape2 = 1 - n1 + n) ## [1] 0.1465283 0.1551559 4.3.1.2 Práctica en STAN En STAN es posible obtener el mismo tipo de inferencia creando cuatro cadenas cuya distribución de probabilidad coincide con la distribución posterior del ejemplo. data { // Entrada el modelo int&lt;lower=0&gt; n; // Numero de observaciones int y[n]; // Vector de longitud n real a; real b; } parameters { // Definir parámetro real&lt;lower=0, upper=1&gt; theta; } model { // Definir modelo y ~ bernoulli(theta); theta ~ beta(a, b); // Distribución previa } generated quantities { real ypred[n]; // vector de longitud n for (ii in 1:n){ ypred[ii] = bernoulli_rng(theta); } } Para compilar STAN debemos definir los parámetros de entrada sample_data &lt;- list(n = nrow(datay), y = datay$y, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan library(rstan) Bernoulli &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/1Bernoulli.stan&quot; options(mc.cores = parallel::detectCores()) model_Bernoulli &lt;- stan( file = Bernoulli, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Bernoulli, file = &quot;Recursos/Día1/Sesion4/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) model_Bernoulli &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Ber1 &lt;- summary(model_Bernoulli, pars = &quot;theta&quot;)$summary tabla_Ber1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.1508 1e-04 0.0024 0.146 0.1492 0.1507 0.1523 0.1554 717.7288 1.0035 Para observar las cadenas compilamos las lineas de código library(posterior) library(ggplot2) temp &lt;- as_draws_df(as.array(model_Bernoulli,pars = &quot;theta&quot;)) p1 &lt;- ggplot(data = temp, aes(x = theta))+ geom_density(color = &quot;blue&quot;, size = 2) + stat_function(fun = posterior1, args = list(y = datay$y), size = 2) + theme_bw(base_size = 20) + labs(x = latex2exp::TeX(&quot;\\\\theta&quot;), y = latex2exp::TeX(&quot;f(\\\\theta)&quot;)) p1 Figura 4.2: Resultado con STAN (línea azul) y posterior teórica (línea negra) Para validar las cadenas library(bayesplot) library(patchwork) posterior_theta &lt;- as.array(model_Bernoulli, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) Predicción de \\(Y\\) en cada una de las iteraciones de las cadenas. y_pred_B &lt;- as.array(model_Bernoulli, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, 1:n] ppc_dens_overlay(y = datay$y, y_pred2) 4.3.2 Modelo de área: Binomial Cuando se dispone de una muestra aleatoria de variables con distribución Bernoulli \\(Y_1,\\ldots,Y_n\\), la inferencia Bayesiana se puede llevar a cabo usando la distribución Binomial, puesto que es bien sabido que la suma de variables aleatorias Bernoulli \\[ \\begin{equation*} S=\\sum_{i=1}^nY_i \\end{equation*} \\] sigue una distribución Binomial. Es decir: \\[ \\begin{equation} p(S \\mid \\theta)=\\binom{n}{s}\\theta^s(1-\\theta)^{n-s}I_{\\{0,1,\\ldots,n\\}}(s), \\end{equation} \\] Nótese que, cuando \\(n=1\\) la distribución Binomial se convierte en una distribución Bernoulli. Puesto que, el parámetro \\(\\theta\\) es una proporción la distribución natural que modela este tipo de parámetros es la distribución beta la cual se define como: \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] La distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid S \\sim Beta(s+\\alpha,\\beta-s+n) \\end{equation*} \\] Ahora, cuando se tiene una sucesión de variables aleatorias \\(S_1,\\ldots,S_d, \\ldots,S_D\\) independientes y con distribución \\(Binomial(n_d,\\theta_d)\\) para \\(d=1,\\ldots,D\\). La distribución posterior del parámetro de interés \\(\\theta_d\\) es \\[ \\begin{equation*} \\theta_d \\mid s_d \\sim Beta\\left(s_d+\\alpha,\\ \\beta+ n_d- s_d\\right) \\end{equation*} \\] Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[P_{d}=\\frac{\\sum_{U}y_{di}}{N_{d}}\\]. Un estimador aproximadamente insesgado para \\(P_{d}\\) basado en el diseño muestral es \\[ \\hat{P}^{DIR}_{d} = \\frac{\\sum_{s_{d}}w_{di}y_{di}}{\\sum_{s_{d}}w_{di}} \\] donde \\(w_{di}\\) es el factor de expansión de \\(i-\\)ésimo individuo en el \\(d-\\)ésimo dominio y \\(y_{di}\\) toma los valores de uno o cero. Ahora, dada la naturaleza de \\(P_d\\), es posible asumir que \\(P_{d}\\mid\\hat{P}^{DIR}_{d} \\sim Beta(\\alpha,\\beta)\\). Luego, el estimador bayesiano para \\(P_{d}\\) esta dado por \\(\\tilde{P}_{d}=E\\left(P_{d}\\mid\\hat{P}^{DIR}_{d}\\right)\\) y la varianza del estimador se obtiene como: \\[ Var\\left(\\tilde{P}_{d}\\right) = Var\\left(P_{d}\\mid\\hat{P}_{d}\\right)=E_{\\mathscr{M}}\\left(Var_{\\mathscr{P}}\\left(P_{d}\\mid\\hat{P}_{d}\\right)\\right)+Var_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(P_{d}\\mid\\hat{P}_{d}\\right)\\right) \\] 4.3.2.1 Práctica en STAN Sea \\(S_k\\) el conteo de personas en condición de pobreza en el \\(k-ésimo\\) departamento en la muestra. dataS &lt;- encuesta %&gt;% transmute( dam = dam_ee, y = ifelse(ingcorte &lt; lp, 1,0) ) %&gt;% group_by(dam) %&gt;% summarise(nd = n(), #Número de ensayos Sd = sum(y) #Número de éxito ) tba(dataS) dam nd Sd 1 10150 1024 2 8510 696 3 6961 787 4 9952 1634 5 19625 1794 6 15862 1734 7 15208 2063 8 21532 3090 9 15481 2754 10 12442 1551 11 5045 251 12 6637 249 13 42601 3327 14 10175 1143 15 7888 831 16 8370 1289 Creando código de STAN data { int&lt;lower=0&gt; K; // Número de provincia int&lt;lower=0&gt; n[K]; // Número de ensayos int&lt;lower=0&gt; s[K]; // Número de éxitos real a; real b; } parameters { real&lt;lower=0, upper=1&gt; theta[K]; // theta_d|s_d } model { for(kk in 1:K) { s[kk] ~ binomial(n[kk], theta[kk]); } to_vector(theta) ~ beta(a, b); } generated quantities { real spred[K]; // vector de longitud K for(kk in 1:K){ spred[kk] = binomial_rng(n[kk],theta[kk]); } } Preparando el código de STAN Binomial2 &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/3Binomial.stan&quot; Organizando datos para STAN sample_data &lt;- list(K = nrow(dataS), s = dataS$Sd, n = dataS$nd, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_Binomial2 &lt;- stan( file = Binomial2, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Binomial2, &quot;Recursos/Día1/Sesion4/0Recursos/Binomial/model_Binomial2.rds&quot;) model_Binomial2 &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Binomial/model_Binomial2.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Bin1 &lt;-summary(model_Binomial2, pars = &quot;theta&quot;)$summary tabla_Bin1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta[1] 0.1010 0e+00 0.0030 0.0954 0.0990 0.1010 0.1030 0.1069 5824.110 0.9983 theta[2] 0.0819 0e+00 0.0029 0.0763 0.0800 0.0819 0.0839 0.0876 4572.696 0.9986 theta[3] 0.1132 1e-04 0.0037 0.1059 0.1108 0.1131 0.1156 0.1206 4096.779 0.9983 theta[4] 0.1643 1e-04 0.0037 0.1573 0.1616 0.1643 0.1668 0.1714 4569.480 0.9986 theta[5] 0.0915 0e+00 0.0021 0.0874 0.0900 0.0915 0.0928 0.0958 4942.271 0.9988 theta[6] 0.1093 0e+00 0.0023 0.1047 0.1078 0.1093 0.1109 0.1141 5090.266 0.9985 theta[7] 0.1357 0e+00 0.0028 0.1302 0.1338 0.1358 0.1376 0.1413 6321.156 0.9985 theta[8] 0.1435 0e+00 0.0024 0.1387 0.1419 0.1435 0.1452 0.1485 5777.110 0.9982 theta[9] 0.1779 0e+00 0.0029 0.1722 0.1760 0.1779 0.1798 0.1838 5066.155 0.9984 theta[10] 0.1247 0e+00 0.0029 0.1193 0.1228 0.1247 0.1267 0.1305 4838.865 0.9985 theta[11] 0.0500 0e+00 0.0030 0.0442 0.0478 0.0499 0.0522 0.0560 4610.448 0.9991 theta[12] 0.0377 0e+00 0.0023 0.0333 0.0361 0.0376 0.0392 0.0425 4921.219 0.9986 theta[13] 0.0781 0e+00 0.0013 0.0756 0.0772 0.0781 0.0790 0.0806 4815.239 0.9984 theta[14] 0.1124 0e+00 0.0031 0.1063 0.1103 0.1124 0.1146 0.1185 4437.218 0.9992 theta[15] 0.1054 1e-04 0.0036 0.0987 0.1029 0.1054 0.1078 0.1125 4547.911 0.9988 theta[16] 0.1542 1e-04 0.0039 0.1466 0.1516 0.1541 0.1567 0.1619 5316.024 0.9994 Para validar las cadenas mcmc_areas(as.array(model_Binomial2, pars = &quot;theta&quot;)) mcmc_trace(as.array(model_Binomial2, pars = &quot;theta&quot;)) y_pred_B &lt;- as.array(model_Binomial2, pars = &quot;spred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 200) y_pred2 &lt;- y_pred_B[rowsrandom, ] g1 &lt;- ggplot(data = dataS, aes(x = Sd))+ geom_histogram(aes(y = ..density..)) + geom_density(size = 2, color = &quot;blue&quot;) + labs(y = &quot;&quot;)+ theme_bw(20) g2 &lt;- ppc_dens_overlay(y = dataS$Sd, y_pred2) g1/g2 4.3.3 Modelo de unidad: Normal con media desconocida Suponga que \\(Y_1,\\cdots,Y_n\\) son variables independientes e idénticamente distribuidos con distribución \\(Normal(\\theta,\\sigma^2)\\) con \\(\\theta\\) desconocido pero \\(\\sigma^2\\) conocido. De esta forma, la función de verosimilitud de los datos está dada por \\[ \\begin{align*} p(\\mathbf{Y} \\mid \\theta) &amp;=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(y_i-\\theta)^2\\right\\}I_\\mathbb{R}(y) \\\\ &amp;=(2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\theta)^2\\right\\} \\end{align*} \\] Como el parámetro \\(\\theta\\) puede tomar cualquier valor en los reales, es posible asignarle una distribución previa \\(\\theta \\sim Normal(\\mu,\\tau^2)\\). Bajo este marco de referencia se tienen los siguientes resultados La distribución posterior del parámetro de interés \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta|\\mathbf{Y} \\sim Normal(\\mu_n,\\tau^2_n) \\end{equation*} \\] En donde \\[ \\begin{equation} \\mu_n=\\frac{\\frac{n}{\\sigma^2}\\bar{Y}+\\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}} \\ \\ \\ \\ \\ \\ \\ \\text{y} \\ \\ \\ \\ \\ \\ \\ \\tau_n^2=\\left(\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}\\right)^{-1} \\end{equation} \\] Obejtivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 4.3.3.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% filter(dam_ee == 1, ingcorte&gt;0) %&gt;% transmute( dam_ee , logIngreso = log(ingcorte +1)) #3 media &lt;- mean(dataNormal$logIngreso) Sd &lt;- sd(dataNormal$logIngreso) g1 &lt;- ggplot(dataNormal,aes(x = logIngreso))+ geom_density(size =2, color = &quot;blue&quot;) + stat_function(fun =dnorm, args = list(mean = media, sd = Sd), size =2) + theme_bw(base_size = 20) + labs(y = &quot;&quot;, x = (&quot;Log(Ingreso)&quot;)) g2 &lt;- ggplot(dataNormal, aes(sample = logIngreso)) + stat_qq() + stat_qq_line() + theme_bw(base_size = 20) g1|g2 Creando código de STAN data { int&lt;lower=0&gt; n; // Número de observaciones real y[n]; // LogIngreso real &lt;lower=0&gt; Sigma; // Desviación estándar } parameters { real theta; } model { y ~ normal(theta, Sigma); theta ~ normal(0, 1000); // Distribución previa } generated quantities { real ypred[n]; // Vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,Sigma); } } Preparando el código de STAN NormalMedia &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/4NormalMedia.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), Sigma = sd(dataNormal$logIngreso), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_NormalMedia &lt;- stan( file = NormalMedia, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia, &quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Nor1 &lt;- summary(model_NormalMedia, pars = &quot;theta&quot;)$summary tabla_Nor1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 12.4565 2e-04 0.007 12.4426 12.4518 12.4564 12.4612 12.4699 864.3918 1.0043 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(dataNormal$logIngreso), y_pred2)/ ppc_dens_overlay(y = exp(as.numeric(dataNormal$logIngreso))-1, exp(y_pred2)-1) + xlim(0,2000000) "],["modelos-multiparamétricos.html", "4.4 Modelos multiparamétricos", " 4.4 Modelos multiparamétricos La distribución normal univariada que tiene dos parámetros: la media \\(\\theta\\) y la varianza \\(\\sigma^2\\). La distribución multinomial cuyo parámetro es un vector de probabilidades \\(\\boldsymbol{\\theta}\\). 4.4.1 Modelo de unidad: Normal con media y varianza desconocida Supongamos que se dispone de realizaciones de un conjunto de variables independientes e idénticamente distribuidas \\(Y_1,\\cdots,Y_n\\sim N(\\theta,\\sigma^2)\\). Cuando se desconoce tanto la media como la varianza de la distribución es necesario plantear diversos enfoques y situarse en el más conveniente, según el contexto del problema. En términos de la asignación de las distribuciones previas para \\(\\theta\\) y \\(\\sigma^2\\) es posible: Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son informativas. Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son no informativas. Suponer que la distribución previa para \\(\\theta\\) depende de \\(\\sigma^2\\) y escribirla como \\(p(\\theta \\mid \\sigma^2)\\), mientras que la distribución previa de \\(\\sigma^2\\) no depende de \\(\\theta\\) y se puede escribir como \\(p(\\sigma^2)\\). La distribución previa para el parámetro \\(\\theta\\) será \\[ \\begin{equation*} \\theta \\sim Normal(0,10000) \\end{equation*} \\] Y la distribución previa para el parámetro \\(\\sigma^2\\) será \\[ \\begin{equation*} \\sigma^2 \\sim IG(0.0001,0.0001) \\end{equation*} \\] La distribución posterior condicional de \\(\\theta\\) es \\[ \\begin{equation} \\theta \\mid \\sigma^2,\\mathbf{Y} \\sim Normal(\\mu_n,\\tau_n^2) \\end{equation} \\] En donde las expresiones para \\(\\mu_n\\) y \\(\\tau_n^2\\) están dados previamente. En el siguiente enlace enconará el libro: Modelos Bayesianos con R y STAN donde puede profundizar en el desarrollo matemático de los resultados anteriores. Obejtivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 4.4.1.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% filter(dam_ee == 1, ingcorte&gt;0) %&gt;% transmute(dam_ee, logIngreso = log(ingcorte +1)) Creando código de STAN data { int&lt;lower=0&gt; n; real y[n]; } parameters { real sigma; real theta; } transformed parameters { real sigma2; sigma2 = pow(sigma, 2); } model { y ~ normal(theta, sigma); theta ~ normal(0, 1000); sigma2 ~ inv_gamma(0.001, 0.001); } generated quantities { real ypred[n]; // vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,sigma); } } Preparando el código de STAN NormalMeanVar &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/5NormalMeanVar.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_NormalMedia &lt;- stan( file = NormalMeanVar, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia,&quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia2.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia2.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\sigma^2\\) es: tabla_Nor2 &lt;- summary(model_NormalMedia, pars = c(&quot;theta&quot;, &quot;sigma2&quot;, &quot;sigma&quot;))$summary tabla_Nor2 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 12.4558 2e-04 0.0072 12.4422 12.4509 12.4556 12.4608 12.4700 1615.399 0.9991 sigma2 0.5066 2e-04 0.0068 0.4938 0.5017 0.5065 0.5112 0.5204 1907.921 0.9999 sigma 0.7118 1e-04 0.0047 0.7027 0.7083 0.7117 0.7150 0.7214 1906.898 0.9999 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) posterior_sigma2 &lt;- as.array(model_NormalMedia, pars = &quot;sigma2&quot;) (mcmc_dens_chains(posterior_sigma2) + mcmc_areas(posterior_sigma2) ) / mcmc_trace(posterior_sigma2) posterior_sigma &lt;- as.array(model_NormalMedia, pars = &quot;sigma&quot;) (mcmc_dens_chains(posterior_sigma) + mcmc_areas(posterior_sigma) ) / mcmc_trace(posterior_sigma) y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(exp(dataNormal$logIngreso)-1), y_pred2) + xlim(0,2000000) 4.4.2 Modelo de área: Multinomial En esta sección discutimos el modelamiento bayesiano de datos provenientes de una distribución multinomial que corresponde a una extensión multivariada de la distribución binomial. Suponga que \\(\\textbf{Y}=(Y_1,\\ldots,Y_K)^{T}\\) es un vector aleatorio con distribución multinomial, así, su distribución está parametrizada por el vector \\(\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_K)^{T}\\) y está dada por la siguiente expresión \\[ \\begin{equation} p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})=\\binom{n}{y_1,\\ldots,y_K}\\prod_{k=1}^K\\theta_k^{y_k} \\ \\ \\ \\ \\ \\theta_k&gt;0 \\texttt{ , } \\sum_{k=1}^{K}y_k=n \\texttt{ y } \\sum_{k=1}^K\\theta_k=1 \\end{equation} \\] Donde \\[ \\begin{equation*} \\binom{n}{y_1,\\ldots,y_K}=\\frac{n!}{y_1!\\cdots y_K!}. \\end{equation*} \\] Como cada parámetro \\(\\theta_k\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible asignar a la distribución de Dirichlet como la distribución previa del vector de parámetros. Por lo tanto la distribución previa del vector de parámetros \\(\\boldsymbol{\\theta}\\), parametrizada por el vector de hiperparámetros \\(\\boldsymbol{\\alpha}=(\\alpha_1,\\ldots,\\alpha_K)^{T}\\), está dada por \\[ \\begin{equation} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha})=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_K)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_K)} \\prod_{k=1}^K\\theta_k^{\\alpha_k-1} \\ \\ \\ \\ \\ \\alpha_k&gt;0 \\texttt{ y } \\sum_{k=1}^K\\theta_k=1 \\end{equation} \\] La distribución posterior del parámetro \\(\\boldsymbol{\\theta}\\) sigue una distribución \\(Dirichlet(y_1+\\alpha_1,\\ldots,y_K+\\alpha_K)\\) 4.4.2.1 Práctica en STAN Sea \\(Y\\) condición de actividad laboral dataMult &lt;- encuesta %&gt;% filter(condact3 %in% 1:3) %&gt;% transmute( empleo = as_factor(condact3)) %&gt;% group_by(empleo) %&gt;% tally() %&gt;% mutate(theta = n/sum(n)) tba(dataMult) empleo n theta Ocupado 92417 0.5291 Desocupado 8671 0.0496 Inactivo 73567 0.4212 donde 1 corresponde a Ocupado, 2 son los Desocupado y 3 son Inactivo Creando código de STAN data { int&lt;lower=0&gt; k; // Número de cátegoria int y[k]; // Número de exitos vector[k] alpha; // Parámetro de las distribción previa } parameters { simplex[k] theta; } transformed parameters { real delta; // Tasa de desocupación delta = theta[2]/ (theta[2] + theta[1]); // (Desocupado)/(Desocupado + Ocupado) } model { y ~ multinomial(theta); theta ~ dirichlet(alpha); } generated quantities { int ypred[k]; ypred = multinomial_rng(theta, sum(y)); } Preparando el código de STAN Multinom &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/6Multinom.stan&quot; Organizando datos para STAN sample_data &lt;- list(k = nrow(dataMult), y = dataMult$n, alpha = c(0.5, 0.5, 0.5)) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_Multinom &lt;- stan( file = Multinom, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_Multinom, &quot;Recursos/Día1/Sesion4/0Recursos/Multinomial/model_Multinom.rds&quot;) model_Multinom &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Multinomial/model_Multinom.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\delta\\) es: tabla_Mul1 &lt;- summary(model_Multinom, pars = c(&quot;delta&quot;, &quot;theta&quot;))$summary tabla_Mul1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat delta 0.0857 0 0.0009 0.0840 0.0851 0.0858 0.0864 0.0875 1201.662 1.0032 theta[1] 0.5291 0 0.0012 0.5267 0.5283 0.5292 0.5300 0.5314 2032.595 1.0001 theta[2] 0.0496 0 0.0005 0.0486 0.0493 0.0496 0.0500 0.0506 1133.188 1.0038 theta[3] 0.4212 0 0.0012 0.4190 0.4205 0.4212 0.4220 0.4236 1874.742 1.0011 posterior_theta1 &lt;- as.array(model_Multinom, pars = &quot;theta[1]&quot;) (mcmc_dens_chains(posterior_theta1) + mcmc_areas(posterior_theta1) ) / mcmc_trace(posterior_theta1) posterior_theta2 &lt;- as.array(model_Multinom, pars = &quot;theta[2]&quot;) (mcmc_dens_chains(posterior_theta2) + mcmc_areas(posterior_theta2) ) / mcmc_trace(posterior_theta2) posterior_theta3 &lt;- as.array(model_Multinom, pars = &quot;theta[3]&quot;) (mcmc_dens_chains(posterior_theta3) + mcmc_areas(posterior_theta3) ) / mcmc_trace(posterior_theta3) posterior_delta &lt;- as.array(model_Multinom, pars = &quot;delta&quot;) (mcmc_dens_chains(posterior_delta) + mcmc_areas(posterior_delta) ) / mcmc_trace(posterior_delta) La imagen es muy pesada no se carga al repositorio. n &lt;- nrow(dataMult) y_pred_B &lt;- as.array(model_Multinom, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 50) y_pred2 &lt;- y_pred_B[, 1:n] ppc_dens_overlay(y = as.numeric(dataMult$n), y_pred2) "],["día-2---sesión-1--estimaciones-casen-y-función-generalizada-de-varianza.html", "Capítulo 5 Día 2 - Sesión 1- Estimaciones CASEN y Función Generalizada de Varianza", " Capítulo 5 Día 2 - Sesión 1- Estimaciones CASEN y Función Generalizada de Varianza Uno de los insumos más importantes en el modelo de áreas es la varianza del estimador directo, a nivel de dominio, la cual no puede calcularse de ningún modo. En correspondencia, este valor debe estimarse desde los datos recolectados en cada dominio. Sin embargo, en dominios en las que se cuenta con un tamaño de muestra muy pequeño, estas estimaciones no tendrán un buen comportamiento. Por ende, es muy útil utilizar un modelo de suavizamiento de las varianzas para eliminar el ruido y la volatilidad de estas estimaciones y extraer la verdadera señal del proceso Hidiroglou (2019) afirma que \\(E_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta}\\) y \\(V_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\sigma_{u}^2+\\tilde{\\sigma}^2_{d}\\), en donde el subíndice \\(\\mathscr{MP}\\) hace referencia a la inferencia doble que se debe tener en cuenta en este tipo de ajustes y define la medida de probabilidad conjunta entre el modelo y el diseño de muestreo. \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento y la inclusión de las covariables auxiliares (\\(\\boldsymbol{x}_{d}\\)). \\(\\mathscr{MP}\\) hace referencia a la medida de probabilidad inducida por el diseño de muestreo complejo que induce las estimaciones directas. La solución que acá se plantea se conoce con el nombre de Función Generalizada de Varianza, la cual consiste en ajustar un modelo log-lineal a la varianza directa estimada. Partiendo del hecho de que se tiene acceso a un estimador insesgado de \\(\\sigma^2\\), denotado por \\(\\hat{\\sigma}^2\\) se tiene que: \\[ E_{\\mathscr{MP}}\\left(\\hat{\\sigma}_{d}^{2}\\right)=E_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(\\hat{\\sigma}_{d}^{2}\\right)\\right)=E_{\\mathscr{M}}\\left(\\sigma_{d}^{2}\\right)=\\tilde{\\sigma}_{d}^{2} \\] La anterior igualdad puede interpretarse como que un estimador insesgado y simple de \\(\\tilde{\\sigma}_{d}^{2}\\) puede ser \\(\\hat{\\sigma}_{d}^{2}\\). Sin embargo, este estimador de muestreo es inestable cuando el tamaño de muestra es pequeño, que es justo el paradigma dominante en la estimación de áreas pequeñas. Rivest and Belmonte (2000) consideran modelos de suavizamiento para la estimación de las varianzas directas definidos de la siguiente manera: \\[ \\log\\left(\\hat{\\sigma}_{d}^{2}\\right)=\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}+\\boldsymbol{\\varepsilon}_{d} \\] En donde \\(\\boldsymbol{z}_{d}\\) es un vector de covariables explicativas que son funciones de \\(\\boldsymbol{x}_{d}\\), \\(\\boldsymbol{\\alpha}\\) es un vector de parámetros que deben ser estimados, \\(\\boldsymbol{\\varepsilon}_{d}\\) son errores aleatorios con media cero y varianza constante, que se asumen idénticamente distribuidos condicionalmente sobre \\(\\boldsymbol{z}_{d}\\). Del anterior modelo, la estimación suavizada de la varianza de muestreo está dada por: \\[ \\tilde{\\sigma}_{d}^{2}=E_{\\mathscr{MP}}\\left(\\sigma_{d}^{2}\\right)=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)\\times\\Delta \\] En donde, \\(E_{\\mathscr{MP}}\\left(\\varepsilon_{d}\\right)=\\Delta\\). No hay necesidad de especificar una distribución paramétrica para los errores de este modelo. Al utilizar el método de los momentos, se tiene el siguiente estimador insesgado para \\(\\Delta\\): \\[ \\hat{\\Delta}=\\frac{\\sum_{d=1}^{D}\\hat{\\sigma}_{d}^{2}}{\\sum_{d=1}^{D}\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)} \\] De la misma forma, al utilizar los procedimientos estándar en una regresión lineal, la estimación del coeficiente de parámetros de regresión está dada por la siguiente expresión: \\[ \\hat{\\boldsymbol{\\alpha}}=\\left(\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\boldsymbol{z}_{d}^{T}\\right)^{-1}\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\log\\left(\\hat{\\sigma}_{d}^{2}\\right) \\] Por último, el estimador suavizado de la varianza muestral está definido por: \\[ \\hat{\\tilde{\\sigma}}_{d}^{2}=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\hat{\\boldsymbol{\\alpha}}\\right)\\hat{\\Delta} \\] "],["datos-de-la-encuesta.html", "5.1 Datos de la encuesta", " 5.1 Datos de la encuesta El siguiente bloque de código utiliza varias librerías en R (tidyverse y magrittr), así como también utiliza una función definida en otro archivo (source(“Recursos/Día2/Sesion1/0Recursos/0Source_FH.R”)). Luego, el código carga la encuesta que esta almacenada en un archivo de datos en formato RDS y utiliza la función %&gt;% para encadenar una serie de transformaciones en los datos: transmute() se utiliza para seleccionar y renombrar columnas. Se crea una nueva variable llamada pobreza que se establece en 1 si la variable ingcorte(ingreso percapital) es menor que la variable lp, y en 0 en caso contrario. La función ifelse() se utiliza para asignar valores a la variable “pobreza” en función de si el ingreso de un individuo es menor o mayor que el umbral de pobreza. library(tidyverse) library(magrittr) source(&quot;Recursos/Día2/Sesion1/0Recursos/0Source_FH.R&quot;) encuesta &lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/encuesta2017CHL.Rds&quot;) %&gt;% transmute( dam = haven::as_factor(dam_ee ,levels = &quot;values&quot;), dam2 = haven::as_factor(comuna,levels = &quot;values&quot;), dam = str_pad(dam, width = 2, pad = &quot;0&quot;), dam2 = str_pad(dam2, width = 5, pad = &quot;0&quot;), wkx = `_fep`, upm = `_upm`, estrato = `_estrato`, pobreza = ifelse(ingcorte &lt; lp, 1 , 0)) dam: Corresponde al código asignado a la división administrativa mayor del país. dam2: Corresponde al código asignado a la segunda división administrativa del país. lp linea de pobreza definida por CEPAL. Factor de expansión por persona (wkx) dam dam2 wkx upm estrato pobreza 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 1 01 01101 39 1100100001 11001 1 01 01101 39 1100100001 11001 1 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 En el siguiente bloque de código utiliza las librerías survey y srvyr para crear un diseño de muestreo a partir de una base de datos de encuestas. El diseño de muestreo incluye información sobre las unidades primarias de muestreo (UPM), los pesos de muestreo (wkx), y las estratas (estrato) utilizadas en el muestreo. Además, se utiliza la opción “survey.lonely.psu” para ajustar los tamaños de muestra en los grupos de unidades primarias de muestreo que no tienen otras unidades primarias de muestreo en el mismo grupo. library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- as_survey_design( ids = upm, weights = wkx, strata = estrato, nest = TRUE, .data = encuesta ) #summary(diseno) Para la estimación directa de la proporción se emplea la función direct.supr, disponible en el archivo 0Source_FH.R. Está función realiza las estimaciones y criterios de calidad en una encuesta de muestreo complejo con diseño estratificado y por conglomerados. Toma cinco argumentos: design.base, variable, group, upm y estrato. La función comienza cargando varios paquetes, como rlang, tidyverse, dplyr, survey y srvyr. Luego, los argumentos group, variable, upm y estrato se convierten en argumentos utilizando la función enquo. La función utiliza la encuesta de muestreo complejo design.base para calcular las estimaciones de los parámetros y los criterios de calidad. Utiliza la función survey_mean() de la librería survey para calcular la media y los intervalos de confianza de la variable de interés. La función también calcula otros indicadores de calidad, como el coeficiente de variación, el tamaño de muestra efectivo y el efecto del diseño. Luego, utiliza la función as.data.frame() para convertir los resultados en un objeto de marco de datos. Además, la función calcula otros criterios de calidad para determinar si las estimaciones son confiables. En particular, evalúa si se cumple un umbral mínimo para el número de grados de libertad, si la muestra es suficientemente grande y si el efecto del diseño es razonable. La función también tiene la opción de incluir o excluir ciertos grupos de muestreo basados en sus características. directodam2 &lt;- direct.supr(design.base = diseno, variable = pobreza, group = dam2, upm = upm, estrato = estrato) directodam2 %&gt;% group_by(Flag) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% tba() Flag n Incluir 102 Excluir 222 Para los dominios que no son excluidos se hace la transformación arcoseno, calculo del DEFF y varianza base_sae &lt;- directodam2 %&gt;% filter(Flag != &quot;Excluir&quot;) %&gt;% transmute( dam2 = dam2, # Id para los dominios nd = n, # Número de observaciones por dominios n_effec = n.eff, # n efectivo. pobreza = p, # Estimación de la variable pobreza_T = asin(sqrt(pobreza)), # Transformación arcoseno vardir = ee ^ 2, # Estimación de la varianza directa cv = CV, var_zd = 1 / (4 * n_effec), # Varianza para la tranformación arcsin deff_dam2 = deff # Deff por dominio ) # View(base_sae) tba(head(base_sae)) dam2 nd n_effec pobreza pobreza_T vardir cv var_zd deff_dam2 01101 6447 1540.5327 0.0764 0.2801 1e-04 13.7671 0.0002 4.1849 01107 3015 874.1965 0.1624 0.4148 4e-04 11.7319 0.0003 3.4489 02101 5473 729.6790 0.0905 0.3055 1e-04 13.0383 0.0003 7.5006 02201 1759 129.0530 0.0772 0.2816 5e-04 28.2624 0.0019 13.6301 03101 3757 717.2133 0.1011 0.3236 3e-04 16.0821 0.0003 5.2383 03301 1221 1165.6410 0.1106 0.3390 1e-04 10.4785 0.0002 1.0475 seguidamente se realiza la transformación \\(\\log(\\hat{\\sigma}^2_d)\\), además se realiza la selección de las columnas identificador del municipio (dam2), la estimación directa (pobreza), El número de personas en el dominio (nd) y la varianza estimada del para la estimación directa vardir,siendo esta la que transforma mediante la función log(). baseFGV &lt;- base_sae %&gt;% select(dam2, pobreza, nd, vardir) %&gt;% mutate(ln_sigma2 = log(vardir)) "],["análisis-gráfico.html", "5.2 Análisis gráfico", " 5.2 Análisis gráfico El primer gráfico, p1, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la variable pobreza, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como pobreza. El segundo gráfico, p2, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la variable nd, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Tamaño de muestra. El tercer gráfico, p3, muestra una gráfica de dispersión de la variable ln_sigma2 en función del producto de pobreza y nd, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Número de pobres. El cuarto gráfico, p4, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la raíz cuadrada de la variable pobreza, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Raiz cuadrada de pobreza. En general, los gráficos estan diseñados para explorar la relación entre ln_sigma2 y diferentes variables independientes, como pobreza, nd, y la raíz cuadrada de la pobreza. La elección de utilizar la función “loess” para suavizar las líneas en lugar de una línea recta puede ayudar a visualizar mejor las tendencias generales en los datos. theme_set(theme_bw()) # pobreza vs Ln_sigma2 # p1 &lt;- ggplot(baseFGV, aes(x = pobreza, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;pobreza&quot;) # Tamaño de muestra vs Ln_sigma2 # p2 &lt;- ggplot(baseFGV, aes(x = nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Tamaño de muestra&quot;) # Número de pobres vs Ln_sigma2 # p3 &lt;- ggplot(baseFGV, aes(x = pobreza * nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Número de pobres&quot;) # Raiz_pobreza vs Ln_sigma2 # p4 &lt;- ggplot(baseFGV, aes(x = sqrt(pobreza), y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Raiz cuadrada de pobreza&quot;) library(patchwork) (p1 | p2) / (p3 | p4) "],["modelo-para-la-varianza.html", "5.3 Modelo para la varianza", " 5.3 Modelo para la varianza El código ajusta un modelo de regresión lineal múltiple (utilizando la función lm()), donde ln_sigma2 es la variable respuesta y las variables predictoras son pobreza, nd, y varias transformaciones de éstas. El objetivo de este modelo es estimar la función generalizada de varianza (FGV) para los dominios observados. library(gtsummary) FGV1 &lt;- lm(ln_sigma2 ~ pobreza + I(nd^2) + I(sqrt(pobreza)), data = baseFGV) tbl_regression(FGV1) %&gt;% add_glance_table(include = c(r.squared, adj.r.squared)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qqdujgczlm .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qqdujgczlm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qqdujgczlm .gt_caption { padding-top: 4px; padding-bottom: 4px; } #qqdujgczlm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qqdujgczlm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #qqdujgczlm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qqdujgczlm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qqdujgczlm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qqdujgczlm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qqdujgczlm .gt_column_spanner_outer:first-child { padding-left: 0; } #qqdujgczlm .gt_column_spanner_outer:last-child { padding-right: 0; } #qqdujgczlm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #qqdujgczlm .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #qqdujgczlm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qqdujgczlm .gt_from_md > :first-child { margin-top: 0; } #qqdujgczlm .gt_from_md > :last-child { margin-bottom: 0; } #qqdujgczlm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qqdujgczlm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #qqdujgczlm .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #qqdujgczlm .gt_row_group_first td { border-top-width: 2px; } #qqdujgczlm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qqdujgczlm .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #qqdujgczlm .gt_first_summary_row.thick { border-top-width: 2px; } #qqdujgczlm .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qqdujgczlm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qqdujgczlm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qqdujgczlm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qqdujgczlm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qqdujgczlm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qqdujgczlm .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #qqdujgczlm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qqdujgczlm .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #qqdujgczlm .gt_left { text-align: left; } #qqdujgczlm .gt_center { text-align: center; } #qqdujgczlm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qqdujgczlm .gt_font_normal { font-weight: normal; } #qqdujgczlm .gt_font_bold { font-weight: bold; } #qqdujgczlm .gt_font_italic { font-style: italic; } #qqdujgczlm .gt_super { font-size: 65%; } #qqdujgczlm .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #qqdujgczlm .gt_asterisk { font-size: 100%; vertical-align: 0; } #qqdujgczlm .gt_indent_1 { text-indent: 5px; } #qqdujgczlm .gt_indent_2 { text-indent: 10px; } #qqdujgczlm .gt_indent_3 { text-indent: 15px; } #qqdujgczlm .gt_indent_4 { text-indent: 20px; } #qqdujgczlm .gt_indent_5 { text-indent: 25px; } Characteristic Beta 95% CI1 p-value pobreza -23 -36, -11 I(nd^2) 0.00 0.00, 0.00 I(sqrt(pobreza)) 23 16, 31 R² 0.612 Adjusted R² 0.600 1 CI = Confidence Interval Después de tener la estimación del modelo se debe obtener el valor de la constante \\(\\Delta\\) para lo cual se usa el siguiente código. delta.hat = sum(baseFGV$vardir) / sum(exp(fitted.values(FGV1))) De donde se obtiene que \\(\\Delta = 1.3509652\\). Final es posible obtener la varianza suavizada ejecutando el siguiente comando. hat.sigma &lt;- data.frame(dam2 = baseFGV$dam2, hat_var = delta.hat * exp(fitted.values(FGV1))) baseFGV &lt;- left_join(baseFGV, hat.sigma) tba(head(baseFGV, 10)) dam2 pobreza nd vardir ln_sigma2 hat_var 01101 0.0764 6447 0.0001 -9.1085 0.0001 01107 0.1624 3015 0.0004 -7.9210 0.0009 02101 0.0905 5473 0.0001 -8.8804 0.0002 02201 0.0772 1759 0.0005 -7.6500 0.0005 03101 0.1011 3757 0.0003 -8.2383 0.0004 03301 0.1106 1221 0.0001 -8.9152 0.0008 04101 0.1710 2719 0.0002 -8.4038 0.0011 04102 0.1930 2498 0.0005 -7.6308 0.0012 04203 0.1004 482 0.0011 -6.8172 0.0008 04301 0.0902 1323 0.0005 -7.6734 0.0006 Validación del modelo para la FGV par(mfrow = c(2, 2)) plot(FGV1) Comparación entre la varianza estimada versus la pronosticada por la FGV ggplot(baseFGV , aes(y = vardir, x = hat_var)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(x = &quot;FGV&quot;, y = &quot;VarDirEst&quot;) + ylab(&quot;Varianza del Estimador Directo&quot;) Predicción de la varianza suavizada base_sae &lt;- base_sae %&gt;% left_join(hat.sigma, by = &quot;dam2&quot;) El siguiente código utiliza la función mutate() del paquete dplyr para crear nuevas variables de la base de datos base_sae y luego guarda el resultado en un archivo RDS llamado base_FH_2018.rds. En concreto, el código realiza las siguientes operaciones: La variable deff_dam2 se ajusta a 1 cuando es NaN. La variable deff_FGV se calcula a partir de otras dos variables hat_var y vardir. Si vardir es 0, entonces deff_FGV se ajusta a 1. En caso contrario, se divide hat_var por vardir / deff_dam2 para obtener deff_FGV. La variable deff_FGV se regulariza utilizando el criterio MDS: si deff_FGV es menor que 1, se ajusta a 1. Finalmente, se calcula la variable n_eff_FGV dividiendo nd (el tamaño de la muestra) por deff_FGV. base_FH &lt;- base_sae %&gt;% mutate( deff_dam2 = ifelse(is.nan(deff_dam2), 1, deff_dam2), deff_FGV = ifelse( vardir == 0 , 1, hat_var / (vardir / deff_dam2) ), # Criterio MDS para regularizar el DeffFGV deff_FGV = ifelse(deff_FGV &lt; 1, 1, deff_FGV), n_eff_FGV = nd / deff_FGV ) saveRDS(object = base_FH, &quot;Recursos/Día2/Sesion1/Data/base_FH_2017.rds&quot;) "],["día-2---sesión-2--modelo-de-fay-herriot---estimación-de-la-pobreza.html", "Capítulo 6 Día 2 - Sesión 2- Modelo de Fay Herriot - Estimación de la pobreza", " Capítulo 6 Día 2 - Sesión 2- Modelo de Fay Herriot - Estimación de la pobreza El modelo de Fay Herriot, propuesto por Fay y Herriot (1979), es un modelo estadístico de área y es el más comúnmente utilizado, cabe tener en cuenta, que dentro de la metodología de estimación en áreas pequeñas, los modelos de área son los de mayor aplicación, ya que lo más factible es no contar con la información a nivel de individuo, pero si encontrar no solo los datos a nivel de área, sino también información auxiliar asociada a estos datos. Este modelo lineal mixto, fue el primero en incluir efectos aleatorios a nivel de área, lo que implica que la mayoría de la información que se introduce al modelo corresponde a agregaciaciones usualmente, departamentos, regiones, provincias, municipios entre otros, donde las estimaciones que se logran con el modelo se obtienen sobre estas agregaciones o subpoblaciones. Ahora, el modelo Fay Herriot es un modelo que relaciona los indicadores de las áreas \\(\\theta_d\\), donde \\(d\\) varía de 1 a \\(D\\), asumiendo que varían con respecto a un vector de \\(p\\) covariables \\(\\boldsymbol{x}_d\\). El modelo se define mediante la ecuación \\(\\theta_d = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d\\), donde \\(u_d\\) es el término de error o efecto aleatorio, diferente para cada área y se distribuye como \\(u_{d} \\stackrel{ind}{\\sim}\\left(0,\\sigma_{u}^{2}\\right)\\). Sin embargo, los verdaderos valores de los indicadores \\(\\theta_d\\) no son observables. Entonces, se utiliza el estimador directo \\(\\hat{\\theta}^{DIR}_d\\) para estimarlos, lo que conduce a un error de muestreo. Este estimador todavía se considera insesgado bajo el diseño muestral, es decir, \\[ \\hat{\\theta}_d^{DIR} = \\theta + e_d \\] El modelo se ajusta entonces utilizando el término de error debido al muestreo \\(e_d\\), donde \\(e_{d} \\stackrel{ind}{\\sim} \\left(0,\\sigma^2_{e_d}\\right)\\) y las varianzas \\(\\sigma^2_{e_d}\\) se estiman utilizando los microdatos de la encuesta. El modelo FH se reescribe como \\[ \\hat{\\theta}^{DIR}_{d} = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d + e_d \\]. El mejor predictor lineal insesgado (BLUP) bajo el modelo FH viene dado por \\[ \\tilde{\\theta}_{d}^{FH} = \\boldsymbol{x}^{T}{d}\\tilde{\\boldsymbol{\\beta}}+\\tilde{u}_{d} \\], donde \\(\\tilde{u}_d = \\gamma_d\\left(\\hat{\\theta}^{DIR}_{d} - \\boldsymbol{x}^{T}_{d}\\tilde{\\boldsymbol{\\beta}} \\right)\\) y \\(\\gamma_d=\\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_{e_d}}\\). Modelo de área para la estimación de la pobreza Sea \\(P_d\\) la probabilidad de encontrar una persona en condición de pobreza en el \\(d-\\)ésimo dominio de la población. Entonces, el estimador directo de \\(P_d\\) se puede escribir como: \\[ \\hat{P}^{DIR}_{d} = P_d + e_d \\] Ahora bien, \\(P_d\\) se puede modelar de la siguiente manera, \\[ P_d = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d \\] Luego, reescribiendo \\(\\hat{P}^{DIR}_{d}\\) en términos de las dos ecuaciones anteriores tenemos: \\[ \\hat{P}^{DIR}_{d} = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d + e_d \\] Ahora, es posible suponer que \\(\\hat{P}^{DIR}_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta, \\sigma_u^2 +\\sigma_{e_d}^2)\\), \\(\\hat{P}^{DIR}_d \\mid u_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + u_d,\\sigma_{e_d}^2)\\) y \\(u_d \\sim N(0, \\sigma^2_u)\\) Luego, se asumen distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma^2_u\\) \\[ \\begin{eqnarray*} \\beta_p &amp; \\sim &amp; N(0, 10000)\\\\ \\sigma^2_u &amp;\\sim &amp; IG(0.0001, 0.0001) \\end{eqnarray*} \\] por tanto, el estimador bayesiano para \\(P_d\\) esta dado como \\(\\tilde{P}_d = E\\left(P_d\\mid\\hat{P}_d^{DIR}\\right)\\) Predictor óptimo de \\(P_d\\) El predictor óptimo de \\(P_d\\) es \\[E(P_d | \\hat{P}^{DIR}_d) = \\gamma_d\\hat{P}^{DIR}_d + (1-\\gamma_d)\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta\\] con \\(\\gamma_d = \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\). sabemos que \\(\\hat{P}^{DIR}_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta, \\sigma_u^2 +\\sigma_{e_d}^2)\\), \\(\\hat{P}^{DIR}_d \\mid u_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + u_d,\\sigma_{e_d}^2)\\) y \\(u_d \\sim N(0, \\sigma^2_u)\\) Por tanto \\[ \\begin{align*} f(u_d| \\hat{P}^{DIR}_d) \\propto f(\\hat{P}^{DIR}_d | u_d)f(u_d) &amp; = \\frac{1}{\\sigma^2_{e_d}\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{1}{2\\sigma^2_{e_d}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta - u_d)^2}\\right\\} \\frac{1}{\\sigma^2_u\\sqrt{2\\pi}}\\exp\\left\\{- \\frac{1}{2\\sigma^2_u}u_d^2\\right\\}\\\\ &amp; \\propto \\exp\\left\\{-\\frac{u_d^2 - 2u_d(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)}{2\\sigma^2_{e_d}} - \\frac{u_d^2}{2\\sigma^2_u}\\right\\} \\\\ &amp; = \\exp\\left\\{-\\frac{1}{2}\\left[(\\frac{1}{\\sigma^2_{e_d}} + \\frac{1}{\\sigma^2_u})u_d^2 - 2\\frac{\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta}{\\sigma_{e_d}^2}u_d\\right] \\right\\} \\\\ &amp; = \\exp \\left\\{ -\\frac{1}{2\\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}}\\left[u_d^2 - 2\\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)u_d \\right] \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2\\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}}\\left[u_d - \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)\\right]^2 \\right\\} \\\\ &amp; \\propto N(E(u_d|\\hat{P}^{DIR}_d), \\text{Var}(u_d|P^{DIR})) \\end{align*} \\] con \\(E(u_d|\\hat{P}^{DIR}_d) = \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)\\) y \\(\\text{Var}(u_d|P^{DIR}) = \\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\). Por lo tanto se tiene, \\[ \\begin{align*} E(P_d | \\hat{P}^{DIR}_d) = \\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + E(u_d|\\hat{P}^{DIR}_d) &amp; = \\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta) \\\\ &amp; = \\frac{\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\hat{P}^{DIR}_d + \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta \\\\ &amp; = \\gamma_d\\hat{P}^{DIR}_d + (1-\\gamma_d)\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta \\end{align*} \\] "],["procedimiento-de-estimación.html", "6.1 Procedimiento de estimación", " 6.1 Procedimiento de estimación Este código utiliza las librerías tidyverse y magrittr para procesamiento y analizar datos. La función readRDS() es utilizada para cargar un archivo de datos en formato RDS, que contiene las estimaciones directas y la varianza suvizada para la proporción de personas en condición de pobreza correspondientes al año 2018. Luego, se utiliza el operador %&gt;% de la librería magrittr para encadenar la selección de las columnas de interés, que corresponden a los nombres dam2, nd, pobreza, vardir y hat_var. library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/base_FH_2017.rds&quot;) %&gt;% select(dam2, nd, pobreza, vardir, hat_var) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/statelevel_predictors_df_dam2.rds&quot;) %&gt;% mutate_at(.vars = c(&quot;luces_nocturnas&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;modificacion_humana&quot;, &quot;accesibilidad_hospitales&quot;, &quot;accesibilidad_hosp_caminado&quot;), function(x) as.numeric(scale(x))) Ahora, se realiza una unión completa (full_join) entre el conjunto de datos base_FH y los predictores statelevel_predictors_df utilizando la variable dam2 como clave de unión. Se utiliza la función tba() para imprimir las primeras 10 filas y 8 columnas del conjunto de datos resultante de la unión anterior. La unión completa (full_join) combina los datos de ambos conjuntos, manteniendo todas las filas de ambos, y llenando con valores faltantes (NA) en caso de no encontrar coincidencias en la variable de unión (dam2 en este caso). La función tba() imprime una tabla en formato HTML en la consola de R que muestra las primeras 10 filas y 8 columnas del conjunto de datos resultante de la unión. base_FH &lt;- full_join(base_FH, statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[1:10,1:8]) dam2 nd pobreza vardir hat_var area0 area1 etnia2 01101 6447 0.0764 0.0001 0.0001 0.0126 0.9874 0.0016 01107 3015 0.1624 0.0004 0.0009 0.0230 0.9770 0.0011 02101 5473 0.0905 0.0001 0.0002 0.0215 0.9785 0.0010 02201 1759 0.0772 0.0005 0.0005 0.0437 0.9563 0.0004 03101 3757 0.1011 0.0003 0.0004 0.0193 0.9807 0.0006 03301 1221 0.1106 0.0001 0.0008 0.1136 0.8864 0.0004 04101 2719 0.1710 0.0002 0.0011 0.0923 0.9077 0.0004 04102 2498 0.1930 0.0005 0.0012 0.0579 0.9421 0.0003 04203 482 0.1004 0.0011 0.0008 0.2005 0.7995 0.0003 04301 1323 0.0902 0.0005 0.0006 0.2133 0.7867 0.0001 # View(base_FH) "],["preparando-los-insumos-para-stan.html", "6.2 Preparando los insumos para STAN", " 6.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados. Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[1:10,1:8]) dam2 nd pobreza vardir hat_var area0 area1 etnia2 01401 NA NA NA NA 0.3575 0.6425 0.0004 01402 NA NA NA NA 1.0000 0.0000 0.0000 01403 NA NA NA NA 1.0000 0.0000 0.0006 01404 NA NA NA NA 0.5938 0.4062 0.0011 01405 NA NA NA NA 0.5792 0.4208 0.0016 02102 NA NA NA NA 0.0381 0.9619 0.0008 02103 NA NA NA NA 1.0000 0.0000 0.0014 02104 NA NA NA NA 0.1648 0.8352 0.0002 02202 NA NA NA NA 1.0000 0.0000 0.0000 02203 NA NA NA NA 0.4976 0.5024 0.0010 Definir matriz de efectos fijos. Define un modelo lineal utilizando la función formula(), que incluye varias variables predictoras, como la edad, la etnia, la tasa de desocupación, entre otras. Utiliza la función model.matrix() para generar matrices de diseño (Xdat y Xs) a partir de los datos observados (data_dir) y no observados (data_syn) para utilizar en la construcción de modelos de regresión. La función model.matrix() convierte las variables categóricas en variables binarias (dummy), de manera que puedan ser utilizadas. formula_mod &lt;- formula(~ sexo2 + anoest2 + anoest3 + anoest4 + edad2 + edad3 + edad4 + edad5 + etnia1 + etnia2 + tasa_desocupacion + luces_nocturnas + cubrimiento_urbano + pollution_CO + vegetation_NDVI + Elevation + precipitation + population_density + cubrimiento_cultivo + alfabeta) ## Dominios observados Xdat &lt;- model.matrix(formula_mod, data = data_dir) ## Dominios no observados Xs &lt;- model.matrix(formula_mod, data = data_syn) Ahora, se utiliza la función setdiff() para identificar las columnas de Xdat que no están presentes en \\(X_s\\), es decir, las variables que no se encuentran en los datos no observados. A continuación, se crea una matriz temporal (temp) con ceros para las columnas faltantes de \\(X_s\\), y se agregan estas columnas a \\(X_s\\) utilizando cbind(). El resultado final es una matriz Xs con las mismas variables que Xdat, lo que asegura que se puedan realizar comparaciones adecuadas entre los datos observados y no observados en la construcción de modelos de regresión. En general, este código es útil para preparar los datos para su posterior análisis y asegurar que los modelos de regresión sean adecuados para su uso. temp &lt;- setdiff(colnames(Xdat),colnames(Xs)) temp &lt;- matrix( 0, nrow = nrow(Xs), ncol = length(temp), dimnames = list(1:nrow(Xs), temp) ) Xs &lt;- cbind(Xs,temp)[,colnames(Xdat)] Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados y = as.numeric(data_dir$pobreza), # Estimación directa sigma_e = sqrt(data_dir$hat_var) # Error de estimación ) Rutina implementada en STAN data { int&lt;lower=0&gt; N1; // number of data items int&lt;lower=0&gt; N2; // number of data items for prediction int&lt;lower=0&gt; p; // number of predictors matrix[N1, p] X; // predictor matrix matrix[N2, p] Xs; // predictor matrix vector[N1] y; // predictor matrix vector[N1] sigma_e; // known variances } parameters { vector[p] beta; // coefficients for predictors real&lt;lower=0&gt; sigma2_u; vector[N1] u; } transformed parameters{ vector[N1] theta; vector[N1] thetaSyn; vector[N1] thetaFH; vector[N1] gammaj; real&lt;lower=0&gt; sigma_u; thetaSyn = X * beta; theta = thetaSyn + u; sigma_u = sqrt(sigma2_u); gammaj = to_vector(sigma_u ./ (sigma_u + sigma_e)); thetaFH = (gammaj) .* y + (1-gammaj).*thetaSyn; } model { // likelihood y ~ normal(theta, sigma_e); // priors beta ~ normal(0, 100); u ~ normal(0, sigma_u); sigma2_u ~ inv_gamma(0.0001, 0.0001); } generated quantities{ vector[N2] y_pred; for(j in 1:N2) { y_pred[j] = normal_rng(Xs[j] * beta, sigma_u); } } Compilando el modelo en STAN. A continuación mostramos la forma de compilar el código de STAN desde R. En este código se utiliza la librería rstan para ajustar un modelo bayesiano utilizando el archivo 17FH_normal.stan que contiene el modelo escrito en el lenguaje de modelado probabilístico Stan. En primer lugar, se utiliza la función stan() para ajustar el modelo a los datos de sample_data. Los argumentos que se pasan a stan() incluyen el archivo que contiene el modelo (fit_FH_normal), los datos (sample_data), y los argumentos para controlar el proceso de ajuste del modelo, como el número de iteraciones para el período de calentamiento (warmup) y el período de muestreo (iter), y el número de núcleos de la CPU para utilizar en el proceso de ajuste (cores). Además, se utiliza la función parallel::detectCores() para detectar automáticamente el número de núcleos disponibles en la CPU, y se establece la opción mc.cores para aprovechar el número máximo de núcleos disponibles para el ajuste del modelo. El resultado del ajuste del modelo es almacenado en model_FH_normal, que contiene una muestra de la distribución posterior del modelo, la cual puede ser utilizada para realizar inferencias sobre los parámetros del modelo y las predicciones. En general, este código es útil para ajustar modelos bayesianos utilizando Stan y realizar inferencias posteriores. library(rstan) fit_FH_normal &lt;- &quot;Recursos/Día2/Sesion2/Data/modelosStan/17FH_normal.stan&quot; options(mc.cores = parallel::detectCores()) model_FH_normal &lt;- stan( file = fit_FH_normal, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(object = model_FH_normal, file = &quot;Recursos/Día2/Sesion2/Data/model_FH_normal.rds&quot;) Leer el modelo model_FH_normal&lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/model_FH_normal.rds&quot;) 6.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(posterior) library(patchwork) y_pred_B &lt;- as.array(model_FH_normal, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) Análisis gráfico de la convergencia de las cadenas de \\(\\sigma^2_V\\). posterior_sigma2_u &lt;- as.array(model_FH_normal, pars = &quot;sigma2_u&quot;) (mcmc_dens_chains(posterior_sigma2_u) + mcmc_areas(posterior_sigma2_u) ) / mcmc_trace(posterior_sigma2_u) Como método de validación se comparan las diferentes elementos de la estimación del modelo de FH obtenidos en STAN theta &lt;- summary(model_FH_normal,pars = &quot;theta&quot;)$summary %&gt;% data.frame() thetaSyn &lt;- summary(model_FH_normal,pars = &quot;thetaSyn&quot;)$summary %&gt;% data.frame() theta_FH &lt;- summary(model_FH_normal,pars = &quot;thetaFH&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate( thetadir = pobreza, theta_pred = theta$mean, thetaSyn = thetaSyn$mean, thetaFH = theta_FH$mean, theta_pred_EE = theta$sd, Cv_theta_pred = theta_pred_EE/theta_pred ) # Estimación predicción del modelo vs ecuación ponderada de FH p11 &lt;- ggplot(data_dir, aes(x = theta_pred, y = thetaFH)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) # Estimación con la ecuación ponderada de FH Vs estimación sintética p12 &lt;- ggplot(data_dir, aes(x = thetaSyn, y = thetaFH)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) # Estimación con la ecuación ponderada de FH Vs estimación directa p21 &lt;- ggplot(data_dir, aes(x = thetadir, y = thetaFH)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) # Estimación directa Vs estimación sintética p22 &lt;- ggplot(data_dir, aes(x = thetadir, y = thetaSyn)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) (p11+p12)/(p21+p22) Estimación del FH de la pobreza en los dominios NO observados. theta_syn_pred &lt;- summary(model_FH_normal,pars = &quot;y_pred&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate( theta_pred = theta_syn_pred$mean, thetaSyn = theta_pred, thetaFH = theta_pred, theta_pred_EE = theta_syn_pred$sd, Cv_theta_pred = theta_pred_EE/theta_pred) tba(data_syn %&gt;% slice(1:10) %&gt;% select(dam2:hat_var,theta_pred:Cv_theta_pred)) dam2 nd pobreza vardir hat_var theta_pred thetaSyn thetaFH theta_pred_EE Cv_theta_pred 01401 NA NA NA NA 0.0462 0.0462 0.0462 0.0414 0.8959 01402 NA NA NA NA 0.0096 0.0096 0.0096 0.0660 6.9003 01403 NA NA NA NA 0.0218 0.0218 0.0218 0.0667 3.0592 01404 NA NA NA NA 0.0543 0.0543 0.0543 0.0404 0.7446 01405 NA NA NA NA -0.2797 -0.2797 -0.2797 0.0975 -0.3487 02102 NA NA NA NA -0.0539 -0.0539 -0.0539 0.0463 -0.8590 02103 NA NA NA NA -0.5446 -0.5446 -0.5446 0.1605 -0.2948 02104 NA NA NA NA -0.0140 -0.0140 -0.0140 0.0347 -2.4718 02202 NA NA NA NA -0.1508 -0.1508 -0.1508 0.0703 -0.4663 02203 NA NA NA NA -0.0718 -0.0718 -0.0718 0.0463 -0.6455 consolidando las bases de estimaciones para dominios observados y NO observados. estimacionesPre &lt;- bind_rows(data_dir, data_syn) %&gt;% select(dam2, theta_pred) %&gt;% mutate(dam = substr(dam2,1,2)) "],["proceso-de-benchmark.html", "6.3 Proceso de Benchmark", " 6.3 Proceso de Benchmark Del censo extraer el total de personas por DAM2 total_pp &lt;- readRDS(file = &quot;Recursos/Día2/Sesion2/Data/total_personas_dam2.rds&quot;) N_dam_pp &lt;- total_pp %&gt;% group_by(dam) %&gt;% mutate(dam_pp = sum(total_pp) ) tba(N_dam_pp %&gt;% data.frame() %&gt;% slice(1:20)) dam dam2 total_pp dam_pp 01 01101 191468 330558 01 01107 108375 330558 01 01401 15711 330558 01 01402 1250 330558 01 01403 1728 330558 01 01404 2730 330558 01 01405 9296 330558 02 02101 361873 607534 02 02102 13467 607534 02 02103 10186 607534 02 02104 13317 607534 02 02201 165731 607534 02 02202 321 607534 02 02203 10996 607534 02 02301 25186 607534 02 02302 6457 607534 03 03101 153937 286168 03 03102 17662 286168 03 03103 14019 286168 03 03201 12219 286168 Obtener las estimaciones directa por DAM o el nivel de agregación en el cual la encuesta es representativa. En este código, se lee un archivo RDS de una encuesta (encuesta2017CHL.rds) y se utilizan las funciones transmute() para seleccionar y transformar las variables de interés. En primer lugar, se crea una variable dam que corresponde al identificador de la división administrativa mayor de la encuesta. A continuación, se utiliza la columna dam_ee para crear una variable dam, se selecciona la variable dam2 que corresponde al identificador de la división administrativa menor de segundo nivel de la encuesta. Luego, se crea una variable wkx que corresponde al peso de la observación en la encuesta, y una variable upm que corresponde al identificador de la Unidad Prrimaria de Muestreo. Finalmente, se crea una variable pobreza que toma el valor 1 si el ingreso de la vivienda es menor que un umbral lp, y 0 en caso contrario. encuesta &lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/encuesta2017CHL.Rds&quot;)%&gt;% transmute( dam = haven::as_factor(dam_ee ,levels = &quot;values&quot;), dam2 = haven::as_factor(comuna,levels = &quot;values&quot;), dam = str_pad(dam, width = 2, pad = &quot;0&quot;), dam2 = str_pad(dam2, width = 5, pad = &quot;0&quot;), wkx = `_fep`, upm = `_upm`, estrato = `_estrato`, pobreza = ifelse(ingcorte &lt; lp, 1 , 0)) El código está realizando un análisis de datos de encuestas utilizando el paquete survey de R. Primero, se crea un objeto diseno de diseño de encuestas usando la función as_survey_design() del paquete srvyr, que incluye los identificadores de la unidad primaria de muestreo (upm), los pesos (wkx), las estratos (estrato) y los datos de la encuesta (encuesta). Posteriormente, se agrupa el objeto diseno por la variable “Agregado” y se calcula la media de la variable pobreza con un intervalo de confianza para toda la población utilizando la función survey_mean(). El resultado se guarda en el objeto directoDam y se muestra en una tabla. library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- as_survey_design( ids = upm, weights = wkx, strata = estrato, nest = TRUE, .data = encuesta ) directoDam &lt;- diseno %&gt;% group_by(dam) %&gt;% summarise( theta_dir = survey_mean(pobreza, vartype = c(&quot;ci&quot;)) ) tba(directoDam) dam theta_dir theta_dir_low theta_dir_upp 01 0.1036 0.0858 0.1215 02 0.0856 0.0665 0.1047 03 0.1111 0.0902 0.1320 04 0.1598 0.1404 0.1792 05 0.0975 0.0852 0.1097 06 0.1124 0.0957 0.1291 07 0.1297 0.1164 0.1431 08 0.1397 0.1242 0.1552 09 0.1714 0.1565 0.1863 10 0.1262 0.1074 0.1450 11 0.0550 0.0393 0.0706 12 0.0357 0.0256 0.0458 13 0.0820 0.0733 0.0907 14 0.1213 0.1034 0.1392 15 0.1035 0.0865 0.1204 16 0.1507 0.1289 0.1725 Realizar el consolidando información obtenida en 1 y 2. temp &lt;- estimacionesPre %&gt;% inner_join(N_dam_pp ) %&gt;% inner_join(directoDam ) tba(temp %&gt;% slice(1:10)) dam2 theta_pred dam total_pp dam_pp theta_dir theta_dir_low theta_dir_upp 01101 0.0742 01 191468 330558 0.1036 0.0858 0.1215 01107 0.1513 01 108375 330558 0.1036 0.0858 0.1215 02101 0.0924 02 361873 607534 0.0856 0.0665 0.1047 02201 0.0744 02 165731 607534 0.0856 0.0665 0.1047 03101 0.1103 03 153937 286168 0.1111 0.0902 0.1320 03301 0.1365 03 51917 286168 0.1111 0.0902 0.1320 04101 0.1555 04 221054 757586 0.1598 0.1404 0.1792 04102 0.1484 04 227730 757586 0.1598 0.1404 0.1792 04203 0.0878 04 21382 757586 0.1598 0.1404 0.1792 04301 0.1191 04 111272 757586 0.1598 0.1404 0.1792 Con la información organizada realizar el calculo de los pesos para el Benchmark R_dam2 &lt;- temp %&gt;% group_by(dam) %&gt;% summarise( R_dam_RB = unique(theta_dir) / sum((total_pp / dam_pp) * theta_pred) ) %&gt;% left_join(directoDam, by = &quot;dam&quot;) tba(R_dam2 %&gt;% arrange(desc(R_dam_RB))) dam R_dam_RB theta_dir theta_dir_low theta_dir_upp 11 1.5703 0.0550 0.0393 0.0706 12 1.5322 0.0357 0.0256 0.0458 02 1.2448 0.0856 0.0665 0.1047 01 1.1845 0.1036 0.0858 0.1215 09 1.1424 0.1714 0.1565 0.1863 04 1.1367 0.1598 0.1404 0.1792 06 1.1332 0.1124 0.0957 0.1291 05 1.1328 0.0975 0.0852 0.1097 13 1.0874 0.0820 0.0733 0.0907 07 1.0856 0.1297 0.1164 0.1431 15 1.0546 0.1035 0.0865 0.1204 03 1.0169 0.1111 0.0902 0.1320 16 1.0110 0.1507 0.1289 0.1725 10 1.0022 0.1262 0.1074 0.1450 08 0.9891 0.1397 0.1242 0.1552 14 0.9814 0.1213 0.1034 0.1392 calculando los pesos para cada dominio. pesos &lt;- temp %&gt;% mutate(W_i = total_pp / dam_pp) %&gt;% select(dam2, W_i) tba(pesos %&gt;% slice(1:10)) dam2 W_i 01101 0.5792 01107 0.3279 02101 0.5956 02201 0.2728 03101 0.5379 03301 0.1814 04101 0.2918 04102 0.3006 04203 0.0282 04301 0.1469 Realizar la estimación FH Benchmark En este proceso, se realiza la adición de una nueva columna denominada R_dam_RB, que es obtenida a partir de un objeto denominado R_dam2. Posteriormente, se agrega una nueva columna denominada theta_pred_RBench, la cual es igual a la multiplicación de R_dam_RB y theta_pred. Finalmente, se hace un left_join con el dataframe pesos, y se seleccionan únicamente las columnas dam, dam2, W_i, theta_pred y theta_pred_RBench para ser presentadas en una tabla (tba) que muestra únicamente las primeras 10 filas. estimacionesBench &lt;- estimacionesPre %&gt;% left_join(R_dam2, by = c(&quot;dam&quot;)) %&gt;% mutate(theta_pred_RBench = R_dam_RB * theta_pred) %&gt;% left_join(pesos) %&gt;% select(dam, dam2, W_i, theta_pred, theta_pred_RBench) tba(estimacionesBench %&gt;% slice(1:10)) dam dam2 W_i theta_pred theta_pred_RBench 01 01101 0.5792 0.0742 0.0879 01 01107 0.3279 0.1513 0.1792 02 02101 0.5956 0.0924 0.1151 02 02201 0.2728 0.0744 0.0926 03 03101 0.5379 0.1103 0.1121 03 03301 0.1814 0.1365 0.1388 04 04101 0.2918 0.1555 0.1767 04 04102 0.3006 0.1484 0.1687 04 04203 0.0282 0.0878 0.0999 04 04301 0.1469 0.1191 0.1353 Validación: Estimación FH con Benchmark estimacionesBench %&gt;% group_by(dam) %&gt;% summarise(theta_reg_RB = sum(W_i * theta_pred_RBench)) %&gt;% left_join(directoDam, by = &quot;dam&quot;) %&gt;% tba() dam theta_reg_RB theta_dir theta_dir_low theta_dir_upp 01 0.1036 0.1036 0.0858 0.1215 02 0.0856 0.0856 0.0665 0.1047 03 0.1111 0.1111 0.0902 0.1320 04 0.1598 0.1598 0.1404 0.1792 05 0.0975 0.0975 0.0852 0.1097 06 0.1124 0.1124 0.0957 0.1291 07 0.1297 0.1297 0.1164 0.1431 08 0.1397 0.1397 0.1242 0.1552 09 0.1714 0.1714 0.1565 0.1863 10 0.1262 0.1262 0.1074 0.1450 11 0.0550 0.0550 0.0393 0.0706 12 0.0357 0.0357 0.0256 0.0458 13 0.0820 0.0820 0.0733 0.0907 14 0.1213 0.1213 0.1034 0.1392 15 0.1035 0.1035 0.0865 0.1204 16 0.1507 0.1507 0.1289 0.1725 "],["validación-de-los-resultados..html", "6.4 Validación de los resultados.", " 6.4 Validación de los resultados. Este código realiza un análisis de datos y visualización mediante el uso de la librería ggplot2. En particular, el código une dos data frames mediante la función left_join(), agrupa los datos por la variable dam y realiza algunas operaciones para transformar las variables thetaSyn\", thetaFH y theta_pred_RBench. Luego, utiliza la función gather() para organizar los datos en formato largo y los visualiza mediante ggplot(). La visualización resultante muestra puntos de diferentes formas y colores para representar los diferentes métodos de estimación, y dos líneas punteadas que representan los intervalos de confianza superior e inferior para los valores observados en la variable theta_dir. temp &lt;- estimacionesBench %&gt;% left_join( bind_rows( data_dir %&gt;% select(dam2, thetaSyn, thetaFH), data_syn %&gt;% select(dam2, thetaSyn, thetaFH))) %&gt;% group_by(dam) %&gt;% summarise(thetaSyn = sum(W_i * thetaSyn), thetaFH = sum(W_i * theta_pred), theta_RBench = sum(W_i * theta_pred_RBench) ) %&gt;% left_join(directoDam, by = &quot;dam&quot;) %&gt;% mutate(id = 1:n()) temp %&lt;&gt;% gather(key = &quot;Metodo&quot;,value = &quot;Estimacion&quot;, -id, -dam, -theta_dir_upp, -theta_dir_low) ggplot(data = temp, aes(x = id, y = Estimacion, shape = Metodo)) + geom_point(aes(color = Metodo), size = 2) + geom_line(aes(y = theta_dir_low), linetype = 2) + geom_line(aes(y = theta_dir_upp), linetype = 2) + theme_bw(10) + scale_x_continuous(breaks = temp$id, labels = temp$dam) + labs(y = &quot;&quot;, x = &quot;&quot;) "],["mapa-de-pobreza.html", "6.5 Mapa de pobreza", " 6.5 Mapa de pobreza Este es un bloque de código se cargan varios paquetes (sp, sf, tmap) y realiza algunas operaciones. Primero, realiza una unión (left_join) entre las estimaciones de ajustadas por el Benchmarking (estimacionesBench) y las estimaciones del modelo (data_dir, data_syn), utilizando la variable dam2 como clave para la unión. Luego, lee un archivo Shapefile que contiene información geoespacial del país. A continuación, crea un mapa temático (tmap) utilizando la función tm_shape() y agregando capas con la función tm_polygons(). El mapa representa una variable theta_pred_RBench utilizando una paleta de colores llamada “YlOrRd” y establece los cortes de los intervalos de la variable con la variable brks_lp. Finalmente, la función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). library(sp) library(sf) library(tmap) estimacionesBench %&lt;&gt;% left_join( bind_rows( data_dir %&gt;% select(dam2, theta_pred_EE , Cv_theta_pred), data_syn %&gt;% select(dam2, theta_pred_EE , Cv_theta_pred))) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día2/Sesion2/Shape/CHL_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(estimacionesBench, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.1,0.15, 0.2, 0.3, 0.4, 0.6, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;theta_pred_RBench&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) Mapa_lp "],["día-2---sesión-3--modelos-de-área---estimación-de-la-pobreza-y-la-transformación-arcoseno..html", "Capítulo 7 Día 2 - Sesión 3- Modelos de área - Estimación de la pobreza y la transformación ArcoSeno.", " Capítulo 7 Día 2 - Sesión 3- Modelos de área - Estimación de la pobreza y la transformación ArcoSeno. En su concepción más básica, el modelo de Fay-Herriot es una combinación lineal de covariables. Sin embargo, el resultado de esta combinación pueden tomar valores que se salen del rango aceptable en el que puede estar una proporción; es decir, en general el estimador de Fay-Herriot \\(\\theta \\in R\\), mientras que el estimador directo \\(\\theta \\in (0,1)\\). La transformación arcoseno esta dada por: \\[ \\hat{z}_d = arcsin\\left( \\sqrt{ \\hat{\\theta}_d} \\right) \\] donde \\[ Var\\left( \\hat{z}_d \\right) = \\frac{\\widehat{DEFF}_d}{4\\times n_d} = \\frac{1}{4\\times n_{d,efectivo} } \\] El modelo de Fay-Herriot estaría definido de la siguiente forma: \\[ \\begin{eqnarray*} Z_d \\mid \\mu_d,\\sigma^2_d &amp; \\sim &amp; N(\\mu_d, \\sigma^2_d)\\\\ \\mu_d &amp; = &amp; \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d \\\\ \\theta_d &amp; = &amp; \\left(sin(\\mu_d)\\right)^2 \\end{eqnarray*} \\] donde \\(u_d \\sim N(0 , \\sigma^2)\\). Suponga de las distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma_{u}^{2}\\) son dadas por \\[ \\begin{eqnarray*} \\boldsymbol{\\beta} \\sim N\\left(0,1000 \\right)\\\\ \\sigma_{u}^{2} \\sim IG\\left(0.0001,0.0001\\right) \\end{eqnarray*} \\] "],["procedimiento-de-estimación-1.html", "7.1 Procedimiento de estimación", " 7.1 Procedimiento de estimación Lectura de la base de datos que resultó en el paso anterior y selección de las columnas de interés library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/base_FH_2017.rds&quot;) %&gt;% transmute(dam2, ## id dominios pobreza, T_pobreza = asin(sqrt(pobreza)), ## creando zd n_effec = n_eff_FGV, ## n efectivo varhat = 1/(4*n_effec) ## varianza para zd ) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/statelevel_predictors_df_dam2.rds&quot;) %&gt;% mutate_at(.vars = c(&quot;luces_nocturnas&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;modificacion_humana&quot;, &quot;accesibilidad_hospitales&quot;, &quot;accesibilidad_hosp_caminado&quot;), function(x) as.numeric(scale(x))) Uniendo las dos bases de datos. base_FH &lt;- full_join(base_FH, statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[,1:8] %&gt;% head(10)) dam2 pobreza T_pobreza n_effec varhat area0 area1 etnia2 01101 0.0764 0.2801 2143.5413 0.0001 0.0126 0.9874 0.0016 01107 0.1624 0.4148 340.4564 0.0007 0.0230 0.9770 0.0011 02101 0.0905 0.3055 585.8595 0.0004 0.0215 0.9785 0.0010 02201 0.0772 0.2816 128.7029 0.0019 0.0437 0.9563 0.0004 03101 0.1011 0.3236 451.0658 0.0006 0.0193 0.9807 0.0006 03301 0.1106 0.3390 184.2783 0.0014 0.1136 0.8864 0.0004 04101 0.1710 0.4263 144.7157 0.0017 0.0923 0.9077 0.0004 04102 0.1930 0.4548 108.3256 0.0023 0.0579 0.9421 0.0003 04203 0.1004 0.3223 215.8664 0.0012 0.2005 0.7995 0.0003 04301 0.0902 0.3051 112.8311 0.0022 0.2133 0.7867 0.0001 Seleccionando las covariables para el modelo. names_cov &lt;- c( &quot;sexo2&quot; , &quot;anoest2&quot; , &quot;anoest3&quot;, &quot;anoest4&quot;, &quot;edad2&quot; , &quot;edad3&quot; , &quot;edad4&quot; , &quot;edad5&quot; , &quot;etnia1&quot;, &quot;etnia2&quot; , &quot;tasa_desocupacion&quot; , &quot;luces_nocturnas&quot; , &quot;cubrimiento_cultivo&quot; , &quot;alfabeta&quot;, &quot;pollution_CO&quot;, &quot;vegetation_NDVI&quot;, &quot;Elevation&quot;, &quot;precipitation&quot;, &quot;population_density&quot; ) "],["preparando-los-insumos-para-stan-1.html", "7.2 Preparando los insumos para STAN", " 7.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(T_pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[,1:8] %&gt;% slice(1:10)) dam2 pobreza T_pobreza n_effec varhat area0 area1 etnia2 01401 NA NA NA NA 0.3575 0.6425 0.0004 01402 NA NA NA NA 1.0000 0.0000 0.0000 01403 NA NA NA NA 1.0000 0.0000 0.0006 01404 NA NA NA NA 0.5938 0.4062 0.0011 01405 NA NA NA NA 0.5792 0.4208 0.0016 02102 NA NA NA NA 0.0381 0.9619 0.0008 02103 NA NA NA NA 1.0000 0.0000 0.0014 02104 NA NA NA NA 0.1648 0.8352 0.0002 02202 NA NA NA NA 1.0000 0.0000 0.0000 02203 NA NA NA NA 0.4976 0.5024 0.0010 Definir matriz de efectos fijos. ## Dominios observados Xdat &lt;- cbind(inter = 1,data_dir[,names_cov]) ## Dominios no observados Xs &lt;- cbind(inter = 1,data_syn[,names_cov]) Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados y = as.numeric(data_dir$T_pobreza), sigma_e = sqrt(data_dir$varhat) ) Compilando el modelo en STAN library(rstan) fit_FH_arcoseno &lt;- &quot;Recursos/Día2/Sesion3/Data/modelosStan/15FH_arcsin_normal.stan&quot; options(mc.cores = parallel::detectCores()) model_FH_arcoseno &lt;- stan( file = fit_FH_arcoseno, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_FH_arcoseno, &quot;Recursos/Día2/Sesion3/Data/model_FH_arcoseno.rds&quot;) model_FH_arcoseno &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/model_FH_arcoseno.rds&quot;) 7.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(patchwork) library(posterior) y_pred_B &lt;- as.array(model_FH_arcoseno, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) Análisis gráfico de la convergencia de las cadenas de \\(\\sigma^2_u\\). posterior_sigma2_u &lt;- as.array(model_FH_arcoseno, pars = &quot;sigma2_u&quot;) (mcmc_dens_chains(posterior_sigma2_u) + mcmc_areas(posterior_sigma2_u) ) / mcmc_trace(posterior_sigma2_u) Estimación del FH de la pobreza en los dominios observados. theta_FH &lt;- summary(model_FH_arcoseno,pars = &quot;theta&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate(pred_arcoseno = theta_FH$mean, pred_arcoseno_EE = theta_FH$sd, Cv_pred = pred_arcoseno_EE/pred_arcoseno) Estimación del FH de la pobreza en los dominios NO observados. theta_FH_pred &lt;- summary(model_FH_arcoseno,pars = &quot;theta_pred&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate(pred_arcoseno = theta_FH_pred$mean, pred_arcoseno_EE = theta_FH_pred$sd, Cv_pred = pred_arcoseno_EE/pred_arcoseno) "],["mapa-de-pobreza-1.html", "7.3 Mapa de pobreza", " 7.3 Mapa de pobreza El siguiente bloque de código carga los paquetes sp, sf y tmap, y realiza algunas operaciones. Primero, une (rbind) las estimaciones de los dominios observados y los no observados (data_dir, data_syn) y selecciona las variables dam2, pobreza, pred_arcoseno, pred_arcoseno_EE y Cv_pred utilizando la función select(). Luego, lee un archivo Shapefile que contiene información geoespacial del país. A continuación, crea un mapa temático (tmap) utilizando la función tm_shape() y agregando capas con la función tm_polygons(). El mapa representa dos variables llamadas pobreza y pred_arcoseno, utilizando una paleta de colores llamada “YlOrRd” y establece los cortes de los intervalos de las variables con la variable brks_lp. Finalmente, la función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). library(sp) library(sf) library(tmap) data_map &lt;- rbind(data_dir, data_syn) %&gt;% select(dam2, pobreza, pred_arcoseno, pred_arcoseno_EE,Cv_pred ) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día2/Sesion3/Shape/CHL_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(data_map, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.15, 0.3, 0.45, 0.6, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;pobreza&quot;, &quot;pred_arcoseno&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) Mapa_lp "],["mapa-del-coeficiente-de-variación..html", "7.4 Mapa del coeficiente de variación.", " 7.4 Mapa del coeficiente de variación. Ahora, se crea un segundo mapa temático (tmap) llamado Mapa_cv. Utiliza la misma estructura del primer mapa (mapa) creado anteriormente y agrega una capa utilizando la función tm_polygons(). El mapa representa la variable Cv_pred, utilizando una paleta de colores llamada “YlOrRd” y establece el título del mapa con el parámetro title. La función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). Finalmente, el mapa Mapa_cv se muestra en la consola de R. Mapa_cv &lt;- mapa + tm_polygons( c(&quot;Cv_pred&quot;), title = &quot;Mapa de pobreza(cv)&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 2.5) Mapa_cv NOTA: Dado que la estimación del modelo y el error de estimación son pequeño, entonces, el coeficiente de variación no es una buena medida de la calidad de la estimación. "],["día-2---sesión-4--modelos-de-área---estimación-de-la-pobreza-en-familia-beta-y-binomial.html", "Capítulo 8 Día 2 - Sesión 4- Modelos de área - Estimación de la pobreza en familia beta y binomial ", " Capítulo 8 Día 2 - Sesión 4- Modelos de área - Estimación de la pobreza en familia beta y binomial "],["modelos-de-área-con-variable-respuesta-binomial..html", "8.1 Modelos de área con variable respuesta Binomial.", " 8.1 Modelos de área con variable respuesta Binomial. El modelo lineal de Fay-Herriot puede ser reemplazado por un modelo mixto lineal generalizado (GLMM). Esto se puede hacer cuando los datos observados \\(Y_d\\) son inherentemente discretos, como cuando son recuentos (no ponderados) de personas u hogares muestreados con ciertas características. Uno de estos modelos supone una distribución binomial para \\(Y_d\\) con probabilidad de éxito \\(\\theta_d\\), y una logística modelo de regresión para \\(\\theta_d\\) con errores normales en la escala logit. El modelo resultante es \\[ \\begin{eqnarray*} Y_{d}\\mid \\theta_{d},n_{d} &amp; \\sim &amp; Bin\\left(n_{d},\\theta_{d}\\right) \\end{eqnarray*} \\] para \\(d=1,\\dots,D\\) y \\[ \\begin{eqnarray*} logit\\left(\\theta_{d}\\right)=\\log\\left(\\frac{\\theta_{d}}{1-\\theta_{d}}\\right) &amp; = &amp; \\boldsymbol{x}_{d}^{T}\\boldsymbol{\\beta}+u_{d} \\end{eqnarray*} \\] donde \\(u_{d}\\sim N\\left(0,\\sigma_{u}^{2}\\right)\\) y \\(n_{d}\\) es el tamaño de la muestra para el área \\(d\\). El modelo anterior se puede aplicar fácilmente a recuentos de muestras no ponderadas \\(Y_d\\), pero esto ignora cualquier aspecto complejo del diseño de la encuesta. En muestras complejas donde las \\(Y_d\\) son estimaciones ponderadas, surgen dos problemas. En primer lugar, los posibles valores de el \\(Y_d\\) no serán los números enteros \\(0, 1, \\dots , n_d\\) para cualquier definición directa de tamaño de muestra \\(n_d\\). En su lugar, \\(Y_d\\) tomará un valor de un conjunto finito de números desigualmente espaciados determinados por las ponderaciones de la encuesta que se aplican a los casos de muestra en el dominio \\(d\\). En segundo lugar, la varianza muestral de \\(Y_d\\) implícito en la distribución Binomial, es decir, \\(n_d \\times \\theta_d (1-\\theta_d)\\), será incorrecto. Abordamos estos dos problemas al definir un tamaño de muestra efectivo \\(\\tilde{n}_d\\), y un número de muestra efectivo de éxitos \\(\\tilde{Y_d}\\) determinó mantener: (i) la estimación directa \\(\\hat{\\theta}_i\\), de la pobreza y (ii) una estimación de la varianza de muestreo correspondiente,\\(\\widehat{Var}(\\hat{\\theta}_d)\\). Es posible suponer que \\[ \\begin{eqnarray*} \\tilde{n}_{d} &amp; \\sim &amp; \\frac{\\check{\\theta}_{d}\\left(1-\\check{\\theta}_{d}\\right)}{\\widehat{Var}\\left(\\hat{\\theta}_{d}\\right)} \\end{eqnarray*} \\] donde \\(\\check{\\theta}_{d}\\) es una preliminar predicción basada en el modelo para la proporción poblacional \\(\\theta_d\\) y \\(\\widehat{Var}\\left(\\hat{\\theta}_{d}\\right)\\) depende de\\(\\check{\\theta}_{d}\\) a través de una función de varianza generalizada ajustada (FGV). Note que \\(\\tilde{Y}_{d}=\\tilde{n}_{d}\\times\\hat{\\theta}_{d}\\). Suponga de las distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma_{u}^{2}\\) son dadas por \\[ \\begin{eqnarray*} \\boldsymbol{\\beta} \\sim N\\left(0,10000\\right)\\\\ \\sigma_{u}^{2} \\sim IG\\left(0.0001,0.0001\\right) \\end{eqnarray*} \\] 8.1.1 Procedimiento de estimación Lectura de la base de datos que resultó en el paso anterior y selección de las columnas de interés library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/base_FH_2017.rds&quot;) %&gt;% select(dam2, pobreza, n_eff_FGV) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/statelevel_predictors_df_dam2.rds&quot;) %&gt;% mutate_at(.vars = c(&quot;luces_nocturnas&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;modificacion_humana&quot;, &quot;accesibilidad_hospitales&quot;, &quot;accesibilidad_hosp_caminado&quot;), function(x) as.numeric(scale(x))) Uniendo las dos bases de datos. base_FH &lt;- full_join(base_FH,statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[,1:8] %&gt;% head(10)) dam2 pobreza n_eff_FGV area0 area1 etnia2 sexo2 edad2 01101 0.0764 2143.5413 0.0126 0.9874 0.0016 0.5044 0.2374 01107 0.1624 340.4564 0.0230 0.9770 0.0011 0.4998 0.2723 02101 0.0905 585.8595 0.0215 0.9785 0.0010 0.4975 0.2610 02201 0.0772 128.7029 0.0437 0.9563 0.0004 0.4808 0.2399 03101 0.1011 451.0658 0.0193 0.9807 0.0006 0.5022 0.2449 03301 0.1106 184.2783 0.1136 0.8864 0.0004 0.5103 0.2077 04101 0.1710 144.7157 0.0923 0.9077 0.0004 0.5212 0.2507 04102 0.1930 108.3256 0.0579 0.9421 0.0003 0.5175 0.2369 04203 0.1004 215.8664 0.2005 0.7995 0.0003 0.4827 0.2003 04301 0.0902 112.8311 0.2133 0.7867 0.0001 0.5181 0.2199 Seleccionando las covariables para el modelo. names_cov &lt;- c( &quot;sexo2&quot; , &quot;anoest2&quot; , &quot;anoest3&quot;, &quot;anoest4&quot;, &quot;edad2&quot; , &quot;edad3&quot; , &quot;edad4&quot; , &quot;edad5&quot; , &quot;etnia1&quot;, &quot;etnia2&quot; , &quot;tasa_desocupacion&quot; , &quot;luces_nocturnas&quot; , &quot;cubrimiento_cultivo&quot; , &quot;alfabeta&quot;, &quot;pollution_CO&quot;, &quot;vegetation_NDVI&quot;, &quot;Elevation&quot;, &quot;precipitation&quot;, &quot;population_density&quot; ) 8.1.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[1:10,1:8]) dam2 pobreza n_eff_FGV area0 area1 etnia2 sexo2 edad2 01401 NA NA 0.3575 0.6425 0.0004 0.4280 0.2539 01402 NA NA 1.0000 0.0000 0.0000 0.4744 0.2064 01403 NA NA 1.0000 0.0000 0.0006 0.4242 0.2685 01404 NA NA 0.5938 0.4062 0.0011 0.4502 0.1967 01405 NA NA 0.5792 0.4208 0.0016 0.2954 0.1962 02102 NA NA 0.0381 0.9619 0.0008 0.4034 0.2467 02103 NA NA 1.0000 0.0000 0.0014 0.1496 0.2073 02104 NA NA 0.1648 0.8352 0.0002 0.4382 0.2078 02202 NA NA 1.0000 0.0000 0.0000 0.3551 0.2118 02203 NA NA 0.4976 0.5024 0.0010 0.4397 0.2510 Definir matriz de efectos fijos. ## Dominios observados Xdat &lt;- data_dir[,names_cov] ## Dominios no observados Xs &lt;- data_syn[,names_cov] Obteniendo el tamaño de muestra efectivo \\(\\tilde{n}_d\\), y el número de muestra efectivo de éxitos \\(\\tilde{Y_d}\\) n_effec = round(data_dir$n_eff_FGV) y_effect = round((data_dir$pobreza)*n_effec) Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados n_effec = n_effec, y_effect = y_effect # Estimación directa. ) Compilando el modelo en STAN library(rstan) fit_FH_binomial &lt;- &quot;Recursos/Día2/Sesion4/Data/modelosStan/14FH_binomial.stan&quot; options(mc.cores = parallel::detectCores()) model_FH_Binomial &lt;- stan( file = fit_FH_binomial, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_FH_Binomial, file = &quot;Recursos/Día2/Sesion4/Data/model_FH_Binomial.rds&quot;) Leer el modelo model_FH_Binomial &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/model_FH_Binomial.rds&quot;) 8.1.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(patchwork) library(posterior) y_pred_B &lt;- as.array(model_FH_Binomial, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) Análisis gráfico de la convergencia de las cadenas de \\(\\sigma_u\\). posterior_sigma_u &lt;- as.array(model_FH_Binomial, pars = &quot;sigma_u&quot;) (mcmc_dens_chains(posterior_sigma_u) + mcmc_areas(posterior_sigma_u) ) / mcmc_trace(posterior_sigma_u) Estimación del FH de la pobreza en los dominios observados. theta_FH &lt;- summary(model_FH_Binomial,pars = &quot;theta&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate(pred_binomial = theta_FH$mean, pred_binomial_EE = theta_FH$sd, Cv_pred = pred_binomial_EE/pred_binomial) Estimación del FH de la pobreza en los dominios NO observados. theta_FH_pred &lt;- summary(model_FH_Binomial,pars = &quot;thetaLP&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate(pred_binomial = theta_FH_pred$mean, pred_binomial_EE = theta_FH_pred$sd, Cv_pred = pred_binomial_EE/pred_binomial) 8.1.2.2 Mapa de pobreza El mapa muestra el nivel de pobreza en diferentes áreas de Colombia, basado en dos variables, pobreza y pred_binomial. Primero, se cargan los paquetes necesarios sp, sf y tmap. Luego, se lee la información de los datos en R y se combinan utilizando la función rbind(). library(sp) library(sf) library(tmap) data_map &lt;- rbind(data_dir, data_syn) %&gt;% select(dam2, pobreza, pred_binomial, pred_binomial_EE,Cv_pred ) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día2/Sesion4/Shape/CHL_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(data_map, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.025,0.05, 0.1, 0.15, 0.2,0.4, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;pobreza&quot;, &quot;pred_binomial&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) Mapa_lp 8.1.2.3 Mapa del coeficiente de variación. Ahora, se crea un segundo mapa temático (tmap) llamado Mapa_cv. Utiliza la misma estructura del primer mapa (mapa) creado anteriormente y agrega una capa utilizando la función tm_polygons(). El mapa representa la variable Cv_pred, utilizando una paleta de colores llamada “YlOrRd” y establece el título del mapa con el parámetro title. La función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). Finalmente, el mapa Mapa_cv se muestra en la consola de R. Mapa_cv &lt;- mapa + tm_polygons( c(&quot;Cv_pred&quot;), title = &quot;Mapa de pobreza(cv)&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 2.5) Mapa_cv NOTA: Dado que la estimación del modelo y el error de estimación son pequeño, entonces, el coeficiente de variación no es una buena medida de la calidad de la estimación. "],["modelos-de-área-con-variable-respuesta-beta..html", "8.2 Modelos de área con variable respuesta Beta.", " 8.2 Modelos de área con variable respuesta Beta. El modelo beta-logístico fue inicialmente considerado por Jiang y Lahiri (2006b) para un enfoque EBP en uno de sus ejemplos ilustrativos para estimar medias de dominio de población finita. El modelo Fay Herriot beta-logístico estaría dado por las siguientes expresiones \\[ \\begin{eqnarray*} \\hat{p}_{d} \\mid P_d &amp; \\sim &amp; beta(a_d, b_d)\\\\ \\end{eqnarray*} \\] La función del enlace es \\[ \\begin{eqnarray*} logit(P_{d}) \\mid \\boldsymbol{\\beta}, \\sigma^2_u &amp; \\sim &amp; N(\\boldsymbol{x}_d^T\\boldsymbol{\\beta},\\sigma^2_u)\\\\ \\end{eqnarray*} \\] Los parámetros \\(a_d\\) y \\(b_d\\) son estimados así: \\[ \\begin{eqnarray*} a_d &amp;=&amp; P_d \\times \\phi_d\\\\ b_d &amp;=&amp; (1 - P_d) \\times \\phi_d\\\\ \\end{eqnarray*} \\] donde \\[\\phi_d = \\frac{n_d}{\\widehat{DEFF}_d} -1 = n_{d,efecctivo} -1\\] Las distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma^2_u\\) \\[ \\begin{eqnarray*} \\beta_k &amp;\\sim&amp; N(0, 10000)\\\\ \\sigma^2_u &amp;\\sim&amp; IG(0.0001,0.0001) \\end{eqnarray*} \\] 8.2.1 Procedimiento de estimación Lectura de la base de datos que resultó en el paso anterior y selección de las columnas de interés library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/base_FH_2017.rds&quot;) %&gt;% select(dam2, pobreza, n_eff_FGV) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/statelevel_predictors_df_dam2.rds&quot;) %&gt;% mutate_at(.vars = c(&quot;luces_nocturnas&quot;, &quot;cubrimiento_cultivo&quot;, &quot;cubrimiento_urbano&quot;, &quot;modificacion_humana&quot;, &quot;accesibilidad_hospitales&quot;, &quot;accesibilidad_hosp_caminado&quot;), function(x) as.numeric(scale(x))) Uniendo las dos bases de datos. base_FH &lt;- full_join(base_FH,statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[,1:8] %&gt;% head(10)) dam2 pobreza n_eff_FGV area0 area1 etnia2 sexo2 edad2 01101 0.0764 2143.5413 0.0126 0.9874 0.0016 0.5044 0.2374 01107 0.1624 340.4564 0.0230 0.9770 0.0011 0.4998 0.2723 02101 0.0905 585.8595 0.0215 0.9785 0.0010 0.4975 0.2610 02201 0.0772 128.7029 0.0437 0.9563 0.0004 0.4808 0.2399 03101 0.1011 451.0658 0.0193 0.9807 0.0006 0.5022 0.2449 03301 0.1106 184.2783 0.1136 0.8864 0.0004 0.5103 0.2077 04101 0.1710 144.7157 0.0923 0.9077 0.0004 0.5212 0.2507 04102 0.1930 108.3256 0.0579 0.9421 0.0003 0.5175 0.2369 04203 0.1004 215.8664 0.2005 0.7995 0.0003 0.4827 0.2003 04301 0.0902 112.8311 0.2133 0.7867 0.0001 0.5181 0.2199 Seleccionando las covariables para el modelo. names_cov &lt;- c( &quot;sexo2&quot; , &quot;anoest2&quot; , &quot;anoest3&quot;, &quot;anoest4&quot;, &quot;edad2&quot; , &quot;edad3&quot; , &quot;edad4&quot; , &quot;edad5&quot; , &quot;etnia1&quot;, &quot;etnia2&quot; , &quot;tasa_desocupacion&quot; , &quot;luces_nocturnas&quot; , &quot;cubrimiento_cultivo&quot; , &quot;alfabeta&quot;, &quot;pollution_CO&quot;, &quot;vegetation_NDVI&quot;, &quot;Elevation&quot;, &quot;precipitation&quot;, &quot;population_density&quot; ) 8.2.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[,1:8] %&gt;% slice(1:10)) dam2 pobreza n_eff_FGV area0 area1 etnia2 sexo2 edad2 01401 NA NA 0.3575 0.6425 0.0004 0.4280 0.2539 01402 NA NA 1.0000 0.0000 0.0000 0.4744 0.2064 01403 NA NA 1.0000 0.0000 0.0006 0.4242 0.2685 01404 NA NA 0.5938 0.4062 0.0011 0.4502 0.1967 01405 NA NA 0.5792 0.4208 0.0016 0.2954 0.1962 02102 NA NA 0.0381 0.9619 0.0008 0.4034 0.2467 02103 NA NA 1.0000 0.0000 0.0014 0.1496 0.2073 02104 NA NA 0.1648 0.8352 0.0002 0.4382 0.2078 02202 NA NA 1.0000 0.0000 0.0000 0.3551 0.2118 02203 NA NA 0.4976 0.5024 0.0010 0.4397 0.2510 Definir matriz de efectos fijos. ## Dominios observados Xdat &lt;- data_dir[,names_cov] ## Dominios no observados Xs &lt;- data_syn[,names_cov] Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados y = as.numeric(data_dir$pobreza), phi = data_dir$n_eff_FGV - 1 ) Compilando el modelo en STAN library(rstan) fit_FH_beta_logitic &lt;- &quot;Recursos/Día2/Sesion4/Data/modelosStan/16FH_beta_logitc.stan&quot; options(mc.cores = parallel::detectCores()) model_FH_beta_logitic &lt;- stan( file = fit_FH_beta_logitic, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_FH_beta_logitic, file = &quot;Recursos/Día2/Sesion4/Data/model_FH_beta_logitic.rds&quot;) model_FH_beta_logitic &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/model_FH_beta_logitic.rds&quot;) 8.2.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(patchwork) library(posterior) y_pred_B &lt;- as.array(model_FH_beta_logitic, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) Análisis gráfico de la convergencia de las cadenas de \\(\\sigma^2_u\\). posterior_sigma2_u &lt;- as.array(model_FH_beta_logitic, pars = &quot;sigma2_u&quot;) (mcmc_dens_chains(posterior_sigma2_u) + mcmc_areas(posterior_sigma2_u) ) / mcmc_trace(posterior_sigma2_u) Estimación del FH de la pobreza en los dominios observados. theta_FH &lt;- summary(model_FH_beta_logitic,pars = &quot;theta&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate(pred_beta_logit = theta_FH$mean, pred_beta_logit_EE = theta_FH$sd, Cv_pred = pred_beta_logit_EE/pred_beta_logit) Estimación del FH de la pobreza en los dominios NO observados. theta_FH_pred &lt;- summary(model_FH_beta_logitic,pars = &quot;thetapred&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate(pred_beta_logit = theta_FH_pred$mean, pred_beta_logit_EE = theta_FH_pred$sd, Cv_pred = pred_beta_logit_EE/pred_beta_logit) 8.2.2.2 Mapa de pobreza El mapa muestra el nivel de pobreza en diferentes áreas de Colombia, basado en dos variables, pobreza y pred_beta_logit. Primero, se cargan los paquetes necesarios sp, sf y tmap. Luego, se lee la información de los datos en R y se combinan utilizando la función rbind(). library(sp) library(sf) library(tmap) data_map &lt;- rbind(data_dir, data_syn) %&gt;% select(dam2, pobreza, pred_beta_logit, pred_beta_logit_EE,Cv_pred ) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día2/Sesion4/Shape/CHL_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(data_map, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.025,0.05, 0.1, 0.15, 0.2,0.4, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;pobreza&quot;, &quot;pred_beta_logit&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) Mapa_lp 8.2.2.3 Mapa del coeficiente de variación. Ahora, se crea un segundo mapa temático (tmap) llamado Mapa_cv. Utiliza la misma estructura del primer mapa (mapa) creado anteriormente y agrega una capa utilizando la función tm_polygons(). El mapa representa la variable Cv_pred, utilizando una paleta de colores llamada “YlOrRd” y establece el título del mapa con el parámetro title. La función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). Finalmente, el mapa Mapa_cv se muestra en la consola de R. Mapa_cv &lt;- mapa + tm_polygons( c(&quot;Cv_pred&quot;), title = &quot;Mapa de pobreza(cv)&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 2.5) Mapa_cv NOTA: Dado que la estimación del modelo y el error de estimación son pequeño, entonces, el coeficiente de variación no es una buena medida de la calidad de la estimación. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
