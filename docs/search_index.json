[["index.html", "Desagregación de datos en encuestas de hogares: metodologías bayesianas para modelos de estimación en áreas pequeñas Agenda", " Desagregación de datos en encuestas de hogares: metodologías bayesianas para modelos de estimación en áreas pequeñas Andrés Gutiérrez1, Stalyn Guerrero2 2023-03-30 Agenda Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["material-del-curso.html", "Material del curso", " Material del curso En el siguiente enlace encontrará material bibliográfico complementario (Libros, presentaciones, casos de estudio y manuales de instalación) Descargar En el siguiente enlace encontrará las rutinas de R desarrolladas para el taller. Descargar "],["día-1---sesión-1--no-dejar-a-nadie-atrás---ods-y-la-agenda-2030.html", "Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030", " Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030 Ver presentación "],["día-1---sesión-2--censo-e-información-satelital.html", "Capítulo 2 Día 1 - Sesión 2- Censo e información satelital ", " Capítulo 2 Día 1 - Sesión 2- Censo e información satelital "],["uso-de-imágenes-satelitales-y-sae.html", "2.1 Uso de imágenes satelitales y SAE", " 2.1 Uso de imágenes satelitales y SAE Uno de los artículo pioneros de estimación de áreas pequeñas fue el artículo de Singh, R, et. al. (2002) el cual abordó la estimación del rendimiento de cultivos para los tehsil (unidad subadministrativa) del distriyo Rohtak district en Haryana (India). Las imágenes raster representan el mundo mediante un conjunto de celdas contiguas igualmente espaciadas conocidas como pixeles, estas imágenes tienen información como un sistema de información geográfico, Un sistema de referencia de coordenadas. Las imágenes almacenan un identificador, un valor en cada pixel (o un vector con diferentes valores) y cada celda tiene asociada una escala de colores. Las imágenes pueden obtenerse crudas y procesadas, estas primeras contienen solamente las capas de colores, las segundas contienen también valores que han sido procesados en cada celda (índices de vegetación, intensidad lumínica, tipo de vegetación). La información cruda puede utilizarse para entrenar características que se desean entrenar (carreteras, tipo de cultivo, bosque / no bosque), afortunadamente en Google Earth Engine encontramos muchos indicadores procesadas asociadas a un pixel. Estos indicadores pueden agregarse a nivel de un área geográfica. 2.1.1 Fuentes de datos de imágenes satelitales Algunas de las principales fuentes de imágenes satelitales son: http://earthexplorer.usgs.gov/ https://lpdaacsvc.cr.usgs.gov/appeears/ https://search.earthdata.nasa.gov/search https://scihub.coGTMnicus.eu/ https://aws.amazon.com/public-data-sets/landsat/ Sin embargo la mayor parte de estas fuentes están centralizadas en Google Earth Engine que permite buscar fuentes de datos provenientes de imágenes satelitales. GEE se puede manejar por medio de APIS en diferentes lenguajes de programación: Javascript (por defecto), Python y R (paquete rgee). "],["google-earth-eninge.html", "2.2 Google Earth Eninge", " 2.2 Google Earth Eninge Crear una cuenta en link, una vez que se ingrese a la cuenta puede buscarse los conjuntos de datos de interés: Una vez se busque el conjunto de datos se puede abrir un editor de código brindado por google en Javascript. Copiar y pegar la sintaxis que brinda el buscador de conjunto de datos para visualizar la imagen raster y disponer de sentencias que GTMmitan la obtención del conjunto de datos de interés posteriormente en R "],["instalación-de-rgee.html", "2.3 Instalación de rgee", " 2.3 Instalación de rgee Descargar e instalar anaconda o conda. (https://www.anaconda.com/products/individual) Abrir Anaconda prompt y configurar ambiente de trabajo (ambiente python rgee_py) con las siguientes sentencias: conda create -n rgee_py python=3.9 activate rgee_py pip install google-api-python-client pip install earthengine-api pip install numpy Listar los ambientes de Python disponibles en anaconda prompt conda env list Una vez identificado la ruta del ambiente ambiente rgee_py definirla en R (no se debe olvidar cambiar \\ por /). Instalar reticulate y rgee, cargar paquetes para procesamiento espacial y configurar el ambiente de trabajo como sigue: library(reticulate) # Conexión con Python library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(dplyr) # Paquete para procesamiento de datos rgee_environment_dir = &quot;C://Users//sguerrero//Anaconda3//envs//rgee_py//python.exe&quot; # Configurar python (Algunas veces no es detectado y se debe reiniciar R) reticulate::use_python(rgee_environment_dir, required=T) rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) Una vez configurado el ambiente puede iniciarlizarse una sesión de Google Earth Engine como sigue: rgee::ee_Initialize(drive = T) Notas: Se debe inicializar cada sesión con el comando rgee::ee_Initialize(drive = T). Los comandos de javascript que invoquen métodos con “.” se sustituyen por signo peso ($), por ejemplo: ee.ImageCollection().filterDate() # Javascript ee$ImageCollection()$filterDate() # R 2.3.1 Descargar información satelital Paso 1: disponer de los shapefile # shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/CHL_dam2.shp&quot;) shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/CHL_dam.shp&quot;) plot(shape[&quot;geometry&quot;]) Paso 2: Seleccionar el archivo de imágenes que desea procesar, para nuestro ejemplo luces nocturnas. luces &lt;- ee$ImageCollection(&quot;NOAA/DMSP-OLS/NIGHTTIME_LIGHTS&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2013-01-01&quot;, &quot;2014-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;stable_lights&quot;)) %&gt;% ee$ImageCollection$toBands() Paso 3: Descargar la información ## Tiempo 10 minutos shape_luces &lt;- map(unique(shape$dam), ~tryCatch(ee_extract( x = luces, y = shape[&quot;dam&quot;] %&gt;% filter(dam == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(dam = .x), error = function(e)data.frame(dam = .x))) shape_luces %&lt;&gt;% bind_rows() tba(shape_luces, cap = &quot;Promedio de luces nocturnasa&quot;) Tabla 2.1: Promedio de luces nocturnasa dam luces_nocturnas 05 0.2383 12 0.0731 11 0.0348 10 0.6748 15 0.4691 01 0.5006 02 0.6402 03 0.4240 04 1.2168 13 10.6203 14 0.9825 09 1.4667 08 2.6897 16 1.9556 07 1.8539 06 3.5849 Repetir la rutina para: Tipo de suelo: crops-coverfraction (Porcentaje de cubrimiento cultivos) y urban-coverfraction (Porcentaje de cobertura urbana) disponibles en https://develoGTMs.google.com/earth-engine/datasets/catalog/COGTMNICUS_Landcover_100m_Proba-V-C3_Global#description Tiempo de viaje al hospital o clínica más cercana (accessibility) y tiempo de viaje al hospital o clínica más cercana utilizando transporte no motorizado (accessibility_walking_only) información disponible en https://develoGTMs.google.com/earth-engine/datasets/catalog/Oxford_MAP_accessibility_to_healthcare_2019 Modificación humana, donde se consideran los asentamiento humano, la agricultura, el transporte, la minería y producción de energía e infraestructura eléctrica. En el siguiente link encuentra la información satelital https://develoGTMs.google.com/earth-engine/datasets/catalog/CSP_HM_GlobalHumanModification#description Paso 4 consolidar la información. dam luces_nocturnas cubrimiento_cultivo cubrimiento_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado 05 0.2383 7.5529 2.1488 0.3131 101.1488 481.0220 12 0.0731 0.0014 0.0486 0.0419 557.4796 2410.1264 11 0.0348 0.4897 0.0455 0.0446 472.6701 1706.0858 10 0.6748 6.2825 0.4273 0.1569 182.5853 530.2074 15 0.4691 0.3049 0.3165 0.0790 129.5948 634.6007 01 0.5006 0.0630 0.1962 0.0576 149.3803 706.6626 02 0.6402 0.0106 0.1421 0.0655 239.0077 993.2293 03 0.4240 0.0887 0.2277 0.0702 261.4221 1033.4120 04 1.2168 1.2978 0.4280 0.1556 157.2359 558.3460 13 10.6203 11.2553 5.3412 0.3404 108.0762 288.9854 Los resultados se muestran en los siguientes mapas 2.3.2 Luces nocturnas 2.3.3 Cubrimiento cultivos 2.3.4 Cubrimiento urbanos 2.3.5 Modificación humana 2.3.6 Tiempo promedio al hospital 2.3.7 Tiempo promedio al hospital en vehiculo no motorizado "],["censos-de-población-y-vivienda.html", "2.4 Censos de población y vivienda", " 2.4 Censos de población y vivienda Es necesario definir las variables del país con los que se desea trabajar. De acuerdo a esto, como primer paso se debe tener acceso al censo del país, para ello puede acceder desde el siguiente enlace https://redatam.org/en/microdata en el cual dispondrá de un archivo .zip con los microdatos del país. Ahora bien, para leer el conjunto de datos, es necesario emplear la función redatam.open de la librería redatam, la cual depende directamente del diccionario censal del software REDATAM, este es un archivo con extensión dicx y que debe encontrarse en la carpeta sobre los datos que se están leyendo. Así, es como se crea un objeto dentro de R que hace la respectiva unión del diccionario con los microdatos de la base de datos censal. La siguiente sintaxis muestra la lectura del diccionario en R y los cálculos iniciales library(redatam) chile &lt;- redatam.open(&quot;CHL/2017/1.Ingreso/Data/cpv2017chl-cde.dicx&quot;) CONTEOS &lt;- redatam.query( CHL, &quot;freq REGION.IDREGION by AREAUR.URBRUR by PERSONA.P09 by PERSONA.ANEST by PERSONA.P08 by PERSONA.PBLOPER&quot;, tot.omit = FALSE ) # Eliminando totales de la tabla CONTEOS2 &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(. != &quot;__tot__&quot;)) Después de realizar algunas validaciones se estandarizan las variables como muestra el siguiente código. censo_mrp &lt;- CONTEOS2 %&gt;% transmute( dam = str_pad( string = IDREGION1_value, width = 2, pad = &quot;0&quot; ), area = case_when(URBRUR2_value == 1 ~ &quot;1&quot;, # 1 = Urbana TRUE ~ &quot;0&quot;), # 0 = Rural sexo = as.character(P085_value), edad = case_when( P093_value %in% 0:14 ~ &quot;1&quot;, # 5 a 14 P093_value %in% 15:29 ~ &quot;2&quot;, # 15 a 29 P093_value %in% 30:44 ~ &quot;3&quot;, # 30 a 44 P093_value %in% 45:64 ~ &quot;4&quot;, # 45 a 64 TRUE ~ &quot;5&quot; ), # 65 o mas anoest = case_when( P093_value &lt; 6~ &quot;98&quot;, #No aplica ANEST4_value == 99 ~ &quot;99&quot;, #NS/NR ANEST4_value == 0 ~ &quot;1&quot;, # Sin educacion ANEST4_value %in% c(1:6) ~ &quot;2&quot;, # 1 - 6 ANEST4_value %in% c(7:12) ~ &quot;3&quot;, # 7 - 12 ANEST4_value &gt; 12 ~ &quot;4&quot;, # mas de 12 TRUE ~ &quot;Error&quot; ), etnia = case_when( PBLOPER6_value == 1 ~ &quot;1&quot;, # Indigenas PBLOPER6_value == 2 ~ &quot;2&quot;, # Afro TRUE ~ &quot;3&quot; # Otro ), value ) %&gt;% group_by(dam, area, etnia, sexo, edad, anoest) %&gt;% summarise(n = sum(value)) A partir de la base estandarizada es posible construir algunas covariables para el departamento. censo_mrp &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/censo_mrp_dam.rds&quot;) tasa_censo &lt;- model.matrix(dam ~ -1 +., data = censo_mrp %&gt;% select(-n)) %&gt;% data.frame() %&gt;% mutate(dam = censo_mrp$dam, n = censo_mrp$n) %&gt;% group_by(dam) %&gt;% summarise_all(~weighted.mean(x = .,w = n)) %&gt;% mutate(etnia1 = 1-etnia3-etnia2 ) %&gt;% select(-area0, -anoest98,-etnia3,-n) tba(tasa_censo) dam area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 anoest99 etnia1 01 0.9380 0.0014 0.4924 0.2482 0.2309 0.2118 0.0755 0.1555 0.4839 0.2053 0.0294 0.2409 02 0.9411 0.0009 0.4815 0.2504 0.2418 0.2226 0.0748 0.1379 0.4793 0.2336 0.0364 0.1348 03 0.9104 0.0005 0.4953 0.2289 0.2075 0.2361 0.0982 0.1710 0.4933 0.1879 0.0183 0.1932 04 0.8119 0.0003 0.5132 0.2274 0.2003 0.2372 0.1182 0.1834 0.4781 0.1850 0.0279 0.0855 05 0.9101 0.0002 0.5153 0.2313 0.1970 0.2450 0.1361 0.1599 0.4705 0.2342 0.0293 0.0657 06 0.7439 0.0001 0.5039 0.2104 0.2083 0.2547 0.1191 0.2114 0.4856 0.1611 0.0235 0.0626 07 0.7322 0.0001 0.5104 0.2178 0.2012 0.2539 0.1230 0.2184 0.4842 0.1501 0.0246 0.0468 08 0.8858 0.0001 0.5178 0.2326 0.1978 0.2493 0.1176 0.1869 0.4591 0.2174 0.0210 0.1074 09 0.7089 0.0001 0.5141 0.2254 0.1973 0.2427 0.1258 0.2158 0.4653 0.1678 0.0282 0.3356 10 0.7361 0.0001 0.5060 0.2202 0.2145 0.2454 0.1121 0.2228 0.4724 0.1651 0.0263 0.2760 11 0.7958 0.0001 0.4800 0.2131 0.2336 0.2383 0.0901 0.1990 0.4715 0.1820 0.0208 0.2817 12 0.9190 0.0001 0.4881 0.2217 0.2277 0.2485 0.1164 0.1578 0.4841 0.2409 0.0201 0.2268 13 0.9630 0.0004 0.5132 0.2434 0.2186 0.2364 0.1079 0.1480 0.4468 0.2692 0.0281 0.0973 14 0.7166 0.0001 0.5093 0.2297 0.1934 0.2508 0.1258 0.2068 0.4695 0.1756 0.0319 0.2502 15 0.9167 0.0205 0.5020 0.2425 0.2098 0.2213 0.1087 0.1555 0.4840 0.2088 0.0323 0.3284 16 0.6943 0.0001 0.5161 0.2096 0.1947 0.2648 0.1355 0.2267 0.4748 0.1603 0.0236 0.0464 El indicador es posible definirlo a partir de una variable del censo, haciendo que el proceso seá hace más corto, para este caso es empleada la variable VIVIENDA.P03C, agregada por dam En el primer bloque que código usando la función redatam.query() se realiza el conteo de viviendas por el material del piso. Seguido de esto, eliminamos los registros que no son de interés, por ejemplo, el total en el dam o total nacional, los cuales se identifican dentro de la base con la etiqueta __tot__. El siguiente paso es contar el número de viviendas por dam que tienen los pisos de tierra en el censo (Pobx) y el total de viviendas que respondieron a la pregunta (PobT), para finalmente realizar el cociente de estas dos preguntas. CONTEOS &lt;- redatam.query(chile, &quot;freq REGION.IDREGION by VIVIENDA.P03C&quot;, tot.omit = FALSE) PISO &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(!. %in% c(&quot;__tot__&quot;,&quot;__mv__&quot;) )) tasa_piso &lt;- PISO %&gt;% mutate(Pobx = ifelse(P03C2_value %in% c(5), value, 0), PobT = value) %&gt;% group_by( depto = IDREGION1_value ) %&gt;% summarise(PobT = sum(PobT), Pobx = sum(Pobx)) %&gt;% transmute(depto, piso_tierra = Pobx/PobT) La tabla resultante se muestra a continuación. dam piso_tierra 05 0.0016 12 0.0005 11 0.0012 10 0.0005 15 0.0222 01 0.0179 02 0.0092 03 0.0132 04 0.0110 13 0.0009 14 0.0006 09 0.0029 08 0.0018 16 0.0047 07 0.0061 06 0.0038 El proceso se repite con otras preguntas del censo hasta consolidar la tabla siguiente. predictors_censo_dam &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/predictors_censo_dam.rds&quot;) tba(predictors_censo_dam) dam area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 anoest99 etnia1 piso_tierra material_paredes material_techo rezago_escolar alfabeta tasa_desocupacion 05 0.9101 0.0002 0.5153 0.2313 0.1970 0.2450 0.1361 0.1599 0.4705 0.2342 0.0293 0.0657 0.0016 0.3236 0.7653 0.3365 0.0479 0.0683 12 0.9190 0.0001 0.4881 0.2217 0.2277 0.2485 0.1164 0.1578 0.4841 0.2409 0.0201 0.2268 0.0005 0.2649 0.8785 0.3382 0.0431 0.0402 11 0.7958 0.0001 0.4800 0.2131 0.2336 0.2383 0.0901 0.1990 0.4715 0.1820 0.0208 0.2817 0.0012 0.3167 0.9398 0.2793 0.0795 0.0445 10 0.7361 0.0001 0.5060 0.2202 0.2145 0.2454 0.1121 0.2228 0.4724 0.1651 0.0263 0.2760 0.0005 0.3166 0.9082 0.2491 0.0823 0.0612 15 0.9167 0.0205 0.5020 0.2425 0.2098 0.2213 0.1087 0.1555 0.4840 0.2088 0.0323 0.3284 0.0222 0.2416 0.5142 0.3194 0.0485 0.0743 01 0.9380 0.0014 0.4924 0.2482 0.2309 0.2118 0.0755 0.1555 0.4839 0.2053 0.0294 0.2409 0.0179 0.2913 0.5745 0.3199 0.0394 0.0728 02 0.9411 0.0009 0.4815 0.2504 0.2418 0.2226 0.0748 0.1379 0.4793 0.2336 0.0364 0.1348 0.0092 0.2216 0.5673 0.3545 0.0342 0.0776 03 0.9104 0.0005 0.4953 0.2289 0.2075 0.2361 0.0982 0.1710 0.4933 0.1879 0.0183 0.1932 0.0132 0.3880 0.8275 0.2802 0.0561 0.0957 04 0.8119 0.0003 0.5132 0.2274 0.2003 0.2372 0.1182 0.1834 0.4781 0.1850 0.0279 0.0855 0.0110 0.3256 0.7896 0.2796 0.0624 0.0848 13 0.9630 0.0004 0.5132 0.2434 0.2186 0.2364 0.1079 0.1480 0.4468 0.2692 0.0281 0.0973 0.0009 0.1369 0.5678 0.3851 0.0429 0.0661 14 0.7166 0.0001 0.5093 0.2297 0.1934 0.2508 0.1258 0.2068 0.4695 0.1756 0.0319 0.2502 0.0006 0.3159 0.9292 0.2643 0.0866 0.0688 09 0.7089 0.0001 0.5141 0.2254 0.1973 0.2427 0.1258 0.2158 0.4653 0.1678 0.0282 0.3356 0.0029 0.3044 0.9193 0.2526 0.0896 0.0740 08 0.8858 0.0001 0.5178 0.2326 0.1978 0.2493 0.1176 0.1869 0.4591 0.2174 0.0210 0.1074 0.0018 0.2106 0.8356 0.3099 0.0689 0.0857 16 0.6943 0.0001 0.5161 0.2096 0.1947 0.2648 0.1355 0.2267 0.4748 0.1603 0.0236 0.0464 0.0047 0.2727 0.8988 0.2332 0.0936 0.0829 07 0.7322 0.0001 0.5104 0.2178 0.2012 0.2539 0.1230 0.2184 0.4842 0.1501 0.0246 0.0468 0.0061 0.2498 0.8574 0.2235 0.0917 0.0680 06 0.7439 0.0001 0.5039 0.2104 0.2083 0.2547 0.1191 0.2114 0.4856 0.1611 0.0235 0.0626 0.0038 0.2575 0.8052 0.2401 0.0763 0.0670 2.4.1 Mapas de las variables con información censal. temp2 &lt;- predictors_censo_dam %&gt;% select(-dam) %&gt;% names() temp2 &lt;- paste0(&quot;Recursos/Día1/Sesion2/0Recursos/&quot;, temp2, &quot; .png&quot;) knitr::include_graphics(temp2) "],["día-1---sesión-3--fundamentos-de-la-inferencia-bayesiana-en-r-y-stan.html", "Capítulo 3 Día 1 - Sesión 3- Fundamentos de la inferencia Bayesiana en R y STAN", " Capítulo 3 Día 1 - Sesión 3- Fundamentos de la inferencia Bayesiana en R y STAN El proyecto Manhattan y la estimación desagregada con encuestas de hogares "],["día-1---sesión-4--modelos-sintéticos-simples.html", "Capítulo 4 Día 1 - Sesión 4- Modelos sintéticos simples ", " Capítulo 4 Día 1 - Sesión 4- Modelos sintéticos simples "],["regla-de-bayes.html", "4.1 Regla de Bayes", " 4.1 Regla de Bayes En términos de inferencia para \\(\\boldsymbol{\\theta}\\), es necesario encontrar la distribución de los parámetros condicionada a la observación de los datos. Para este fin, es necesario definir la distribución conjunta de la variable de interés con el vector de parámetros. \\[ p(\\boldsymbol{\\theta},\\mathbf{Y})=p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta}) \\] La distribución \\(p(\\boldsymbol{\\theta})\\) se le conoce con el nombre de distribución previa. El término \\(p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})\\) es la distribución de muestreo, verosimilitud o distribución de los datos. La distribución del vector de parámetros condicionada a los datos observados está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})=\\frac{p(\\boldsymbol{\\theta},\\mathbf{Y})}{p(\\mathbf{Y})}=\\frac{p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{Y})} \\] A la distribución \\(p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\) se le conoce con el nombre de distribución posterior. Nótese que el denominador no depende del vector de parámetros y considerando a los datos observados como fijos, corresponde a una constante y puede ser obviada. Por lo tanto, otra representación de la regla de Bayes está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\propto p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) \\] "],["inferencia-bayesiana..html", "4.2 Inferencia Bayesiana.", " 4.2 Inferencia Bayesiana. En términos de estimación, inferencia y predicción, el enfoque Bayesiano supone dos momentos o etapas: Antes de la recolección de las datos, en donde el investigador propone, basado en su conocimiento, experiencia o fuentes externas, una distribución de probabilidad previa para el parámetro de interés. Después de la recolección de los datos. Siguiendo el teorema de Bayes, el investigador actualiza su conocimiento acerca del comportamiento probabilístico del parámetro de interés mediante la distribución posterior de este. "],["modelos-uniparamétricos.html", "4.3 Modelos uniparamétricos", " 4.3 Modelos uniparamétricos Los modelos que están definidos en términos de un solo parámetro que pertenece al conjunto de los números reales se definen como modelos uniparamétricos. 4.3.1 Modelo de unidad: Bernoulli Suponga que \\(Y\\) es una variable aleatoria con distribución Bernoulli dada por: \\[ p(Y \\mid \\theta)=\\theta^y(1-\\theta)^{1-y}I_{\\{0,1\\}}(y) \\] Como el parámetro \\(\\theta\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible formular varias opciones para la distribución previa del parámetro. En particular, la distribución uniforme restringida al intervalo \\([0,1]\\) o la distribución Beta parecen ser buenas opciones. Puesto que la distribución uniforme es un caso particular de la distribución Beta. Por lo tanto la distribución previa del parámetro \\(\\theta\\) estará dada por \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] y la distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid Y \\sim Beta(y+\\alpha,\\beta-y+1) \\end{equation*} \\] Cuando se tiene una muestra aleatoria \\(Y_1,\\ldots,Y_n\\) de variables con distribución Bernoulli de parámetro \\(\\theta\\), entonces la distribución posterior del parámetro de interés es \\[ \\begin{equation*} \\theta \\mid Y_1,\\ldots,Y_n \\sim Beta\\left(\\sum_{i=1}^ny_i+\\alpha,\\beta-\\sum_{i=1}^ny_i+n\\right) \\end{equation*} \\] Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[ P_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario. Note que, \\[ \\begin{equation*} \\bar{Y}_d = P_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(P\\) esta dado por: \\[ \\hat{P} = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{P} = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 4.3.1.1 Práctica en R library(tidyverse) encuesta &lt;- readRDS(&quot;Recursos/Día1/Sesion4/Data/encuesta2017CHL.Rds&quot;) Sea \\(Y\\) la variable aleatoria \\[ Y_{i}=\\begin{cases} 1 &amp; ingreso&lt;lp\\\\ 0 &amp; ingreso\\geq lp \\end{cases} \\] El tamaño de la muestra es de 26434 Indígena datay &lt;- encuesta %&gt;% filter(etnia_ee == 1) %&gt;% transmute(y = ifelse(ingcorte &lt; lp, 1,0)) addmargins(table(datay$y)) 0 1 Sum 22448 3986 26434 Un grupo de estadístico experto decide utilizar una distribución previa Beta, definiendo los parámetros de la distribución previa como \\(Beta(\\alpha=1, \\beta=1)\\). La distribución posterior del parámetro de interés, que representa la probabilidad de estar por debajo de la linea de pobreza, es \\(Beta(3986 + 1, 1 - 3986 + 26434)=Beta(3987, 2.2449\\times 10^{4})\\) Figura 4.1: Distribución previa (línea roja) y distribución posterior (línea negra) La estimación del parámetro estaría dado por: \\[ E(X) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{3987}{3987+ 2.2449\\times 10^{4}} = 0.1508171 \\] luego, el intervalo de credibilidad para la distribución posterior es. n = length(datay$y) n1 = sum(datay$y) qbeta(c(0.025, 0.975), shape1 = 1 + n1, shape2 = 1 - n1 + n) ## [1] 0.1465283 0.1551559 4.3.1.2 Práctica en STAN En STAN es posible obtener el mismo tipo de inferencia creando cuatro cadenas cuya distribución de probabilidad coincide con la distribución posterior del ejemplo. data { // Entrada el modelo int&lt;lower=0&gt; n; // Numero de observaciones int y[n]; // Vector de longitud n real a; real b; } parameters { // Definir parámetro real&lt;lower=0, upper=1&gt; theta; } model { // Definir modelo y ~ bernoulli(theta); theta ~ beta(a, b); // Distribución previa } generated quantities { real ypred[n]; // vector de longitud n for (ii in 1:n){ ypred[ii] = bernoulli_rng(theta); } } Para compilar STAN debemos definir los parámetros de entrada sample_data &lt;- list(n = nrow(datay), y = datay$y, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan library(rstan) Bernoulli &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/1Bernoulli.stan&quot; options(mc.cores = parallel::detectCores()) model_Bernoulli &lt;- stan( file = Bernoulli, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Bernoulli, file = &quot;Recursos/Día1/Sesion4/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) model_Bernoulli &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Ber1 &lt;- summary(model_Bernoulli, pars = &quot;theta&quot;)$summary tabla_Ber1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.1508 1e-04 0.0024 0.146 0.1492 0.1507 0.1523 0.1554 717.7288 1.0035 Para observar las cadenas compilamos las lineas de código library(posterior) library(ggplot2) temp &lt;- as_draws_df(as.array(model_Bernoulli,pars = &quot;theta&quot;)) p1 &lt;- ggplot(data = temp, aes(x = theta))+ geom_density(color = &quot;blue&quot;, size = 2) + stat_function(fun = posterior1, args = list(y = datay$y), size = 2) + theme_bw(base_size = 20) + labs(x = latex2exp::TeX(&quot;\\\\theta&quot;), y = latex2exp::TeX(&quot;f(\\\\theta)&quot;)) p1 Figura 4.2: Resultado con STAN (línea azul) y posterior teórica (línea negra) Para validar las cadenas library(bayesplot) library(patchwork) posterior_theta &lt;- as.array(model_Bernoulli, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) Predicción de \\(Y\\) en cada una de las iteraciones de las cadenas. y_pred_B &lt;- as.array(model_Bernoulli, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, 1:n] ppc_dens_overlay(y = datay$y, y_pred2) 4.3.2 Modelo de área: Binomial Cuando se dispone de una muestra aleatoria de variables con distribución Bernoulli \\(Y_1,\\ldots,Y_n\\), la inferencia Bayesiana se puede llevar a cabo usando la distribución Binomial, puesto que es bien sabido que la suma de variables aleatorias Bernoulli \\[ \\begin{equation*} S=\\sum_{i=1}^nY_i \\end{equation*} \\] sigue una distribución Binomial. Es decir: \\[ \\begin{equation} p(S \\mid \\theta)=\\binom{n}{s}\\theta^s(1-\\theta)^{n-s}I_{\\{0,1,\\ldots,n\\}}(s), \\end{equation} \\] Nótese que, cuando \\(n=1\\) la distribución Binomial se convierte en una distribución Bernoulli. Puesto que, el parámetro \\(\\theta\\) es una proporción la distribución natural que modela este tipo de parámetros es la distribución beta la cual se define como: \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] La distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid S \\sim Beta(s+\\alpha,\\beta-s+n) \\end{equation*} \\] Ahora, cuando se tiene una sucesión de variables aleatorias \\(S_1,\\ldots,S_d, \\ldots,S_D\\) independientes y con distribución \\(Binomial(n_d,\\theta_d)\\) para \\(d=1,\\ldots,D\\). La distribución posterior del parámetro de interés \\(\\theta_d\\) es \\[ \\begin{equation*} \\theta_d \\mid s_d \\sim Beta\\left(s_d+\\alpha,\\ \\beta+ n_d- s_d\\right) \\end{equation*} \\] Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[P_{d}=\\frac{\\sum_{U}y_{di}}{N_{d}}\\]. Un estimador aproximadamente insesgado para \\(P_{d}\\) basado en el diseño muestral es \\[ \\hat{P}^{DIR}_{d} = \\frac{\\sum_{s_{d}}w_{di}y_{di}}{\\sum_{s_{d}}w_{di}} \\] donde \\(w_{di}\\) es el factor de expansión de \\(i-\\)ésimo individuo en el \\(d-\\)ésimo dominio y \\(y_{di}\\) toma los valores de uno o cero. Ahora, dada la naturaleza de \\(P_d\\), es posible asumir que \\(P_{d}\\mid\\hat{P}^{DIR}_{d} \\sim Beta(\\alpha,\\beta)\\). Luego, el estimador bayesiano para \\(P_{d}\\) esta dado por \\(\\tilde{P}_{d}=E\\left(P_{d}\\mid\\hat{P}^{DIR}_{d}\\right)\\) y la varianza del estimador se obtiene como: \\[ Var\\left(\\tilde{P}_{d}\\right) = Var\\left(P_{d}\\mid\\hat{P}_{d}\\right)=E_{\\mathscr{M}}\\left(Var_{\\mathscr{P}}\\left(P_{d}\\mid\\hat{P}_{d}\\right)\\right)+Var_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(P_{d}\\mid\\hat{P}_{d}\\right)\\right) \\] 4.3.2.1 Práctica en STAN Sea \\(S_k\\) el conteo de personas en condición de pobreza en el \\(k-ésimo\\) departamento en la muestra. dataS &lt;- encuesta %&gt;% transmute( dam = dam_ee, y = ifelse(ingcorte &lt; lp, 1,0) ) %&gt;% group_by(dam) %&gt;% summarise(nd = n(), #Número de ensayos Sd = sum(y) #Número de éxito ) tba(dataS) dam nd Sd 1 10150 1024 2 8510 696 3 6961 787 4 9952 1634 5 19625 1794 6 15862 1734 7 15208 2063 8 21532 3090 9 15481 2754 10 12442 1551 11 5045 251 12 6637 249 13 42601 3327 14 10175 1143 15 7888 831 16 8370 1289 Creando código de STAN data { int&lt;lower=0&gt; K; // Número de provincia int&lt;lower=0&gt; n[K]; // Número de ensayos int&lt;lower=0&gt; s[K]; // Número de éxitos real a; real b; } parameters { real&lt;lower=0, upper=1&gt; theta[K]; // theta_d|s_d } model { for(kk in 1:K) { s[kk] ~ binomial(n[kk], theta[kk]); } to_vector(theta) ~ beta(a, b); } generated quantities { real spred[K]; // vector de longitud K for(kk in 1:K){ spred[kk] = binomial_rng(n[kk],theta[kk]); } } Preparando el código de STAN Binomial2 &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/3Binomial.stan&quot; Organizando datos para STAN sample_data &lt;- list(K = nrow(dataS), s = dataS$Sd, n = dataS$nd, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_Binomial2 &lt;- stan( file = Binomial2, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Binomial2, &quot;Recursos/Día1/Sesion4/0Recursos/Binomial/model_Binomial2.rds&quot;) model_Binomial2 &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Binomial/model_Binomial2.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Bin1 &lt;-summary(model_Binomial2, pars = &quot;theta&quot;)$summary tabla_Bin1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta[1] 0.1010 0e+00 0.0030 0.0954 0.0990 0.1010 0.1030 0.1069 5824.110 0.9983 theta[2] 0.0819 0e+00 0.0029 0.0763 0.0800 0.0819 0.0839 0.0876 4572.696 0.9986 theta[3] 0.1132 1e-04 0.0037 0.1059 0.1108 0.1131 0.1156 0.1206 4096.779 0.9983 theta[4] 0.1643 1e-04 0.0037 0.1573 0.1616 0.1643 0.1668 0.1714 4569.480 0.9986 theta[5] 0.0915 0e+00 0.0021 0.0874 0.0900 0.0915 0.0928 0.0958 4942.271 0.9988 theta[6] 0.1093 0e+00 0.0023 0.1047 0.1078 0.1093 0.1109 0.1141 5090.266 0.9985 theta[7] 0.1357 0e+00 0.0028 0.1302 0.1338 0.1358 0.1376 0.1413 6321.156 0.9985 theta[8] 0.1435 0e+00 0.0024 0.1387 0.1419 0.1435 0.1452 0.1485 5777.110 0.9982 theta[9] 0.1779 0e+00 0.0029 0.1722 0.1760 0.1779 0.1798 0.1838 5066.155 0.9984 theta[10] 0.1247 0e+00 0.0029 0.1193 0.1228 0.1247 0.1267 0.1305 4838.865 0.9985 theta[11] 0.0500 0e+00 0.0030 0.0442 0.0478 0.0499 0.0522 0.0560 4610.448 0.9991 theta[12] 0.0377 0e+00 0.0023 0.0333 0.0361 0.0376 0.0392 0.0425 4921.219 0.9986 theta[13] 0.0781 0e+00 0.0013 0.0756 0.0772 0.0781 0.0790 0.0806 4815.239 0.9984 theta[14] 0.1124 0e+00 0.0031 0.1063 0.1103 0.1124 0.1146 0.1185 4437.218 0.9992 theta[15] 0.1054 1e-04 0.0036 0.0987 0.1029 0.1054 0.1078 0.1125 4547.911 0.9988 theta[16] 0.1542 1e-04 0.0039 0.1466 0.1516 0.1541 0.1567 0.1619 5316.024 0.9994 Para validar las cadenas mcmc_areas(as.array(model_Binomial2, pars = &quot;theta&quot;)) mcmc_trace(as.array(model_Binomial2, pars = &quot;theta&quot;)) y_pred_B &lt;- as.array(model_Binomial2, pars = &quot;spred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 200) y_pred2 &lt;- y_pred_B[rowsrandom, ] g1 &lt;- ggplot(data = dataS, aes(x = Sd))+ geom_histogram(aes(y = ..density..)) + geom_density(size = 2, color = &quot;blue&quot;) + labs(y = &quot;&quot;)+ theme_bw(20) g2 &lt;- ppc_dens_overlay(y = dataS$Sd, y_pred2) g1/g2 4.3.3 Modelo de unidad: Normal con media desconocida Suponga que \\(Y_1,\\cdots,Y_n\\) son variables independientes e idénticamente distribuidos con distribución \\(Normal(\\theta,\\sigma^2)\\) con \\(\\theta\\) desconocido pero \\(\\sigma^2\\) conocido. De esta forma, la función de verosimilitud de los datos está dada por \\[ \\begin{align*} p(\\mathbf{Y} \\mid \\theta) &amp;=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(y_i-\\theta)^2\\right\\}I_\\mathbb{R}(y) \\\\ &amp;=(2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\theta)^2\\right\\} \\end{align*} \\] Como el parámetro \\(\\theta\\) puede tomar cualquier valor en los reales, es posible asignarle una distribución previa \\(\\theta \\sim Normal(\\mu,\\tau^2)\\). Bajo este marco de referencia se tienen los siguientes resultados La distribución posterior del parámetro de interés \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta|\\mathbf{Y} \\sim Normal(\\mu_n,\\tau^2_n) \\end{equation*} \\] En donde \\[ \\begin{equation} \\mu_n=\\frac{\\frac{n}{\\sigma^2}\\bar{Y}+\\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}} \\ \\ \\ \\ \\ \\ \\ \\text{y} \\ \\ \\ \\ \\ \\ \\ \\tau_n^2=\\left(\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}\\right)^{-1} \\end{equation} \\] Obejtivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 4.3.3.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% filter(dam_ee == 1, ingcorte&gt;0) %&gt;% transmute( dam_ee , logIngreso = log(ingcorte +1)) #3 media &lt;- mean(dataNormal$logIngreso) Sd &lt;- sd(dataNormal$logIngreso) g1 &lt;- ggplot(dataNormal,aes(x = logIngreso))+ geom_density(size =2, color = &quot;blue&quot;) + stat_function(fun =dnorm, args = list(mean = media, sd = Sd), size =2) + theme_bw(base_size = 20) + labs(y = &quot;&quot;, x = (&quot;Log(Ingreso)&quot;)) g2 &lt;- ggplot(dataNormal, aes(sample = logIngreso)) + stat_qq() + stat_qq_line() + theme_bw(base_size = 20) g1|g2 Creando código de STAN data { int&lt;lower=0&gt; n; // Número de observaciones real y[n]; // LogIngreso real &lt;lower=0&gt; Sigma; // Desviación estándar } parameters { real theta; } model { y ~ normal(theta, Sigma); theta ~ normal(0, 1000); // Distribución previa } generated quantities { real ypred[n]; // Vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,Sigma); } } Preparando el código de STAN NormalMedia &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/4NormalMedia.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), Sigma = sd(dataNormal$logIngreso), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_NormalMedia &lt;- stan( file = NormalMedia, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia, &quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Nor1 &lt;- summary(model_NormalMedia, pars = &quot;theta&quot;)$summary tabla_Nor1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 12.4565 2e-04 0.007 12.4426 12.4518 12.4564 12.4612 12.4699 864.3918 1.0043 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(dataNormal$logIngreso), y_pred2)/ ppc_dens_overlay(y = exp(as.numeric(dataNormal$logIngreso))-1, exp(y_pred2)-1) + xlim(0,2000000) "],["modelos-multiparamétricos.html", "4.4 Modelos multiparamétricos", " 4.4 Modelos multiparamétricos La distribución normal univariada que tiene dos parámetros: la media \\(\\theta\\) y la varianza \\(\\sigma^2\\). La distribución multinomial cuyo parámetro es un vector de probabilidades \\(\\boldsymbol{\\theta}\\). 4.4.1 Modelo de unidad: Normal con media y varianza desconocida Supongamos que se dispone de realizaciones de un conjunto de variables independientes e idénticamente distribuidas \\(Y_1,\\cdots,Y_n\\sim N(\\theta,\\sigma^2)\\). Cuando se desconoce tanto la media como la varianza de la distribución es necesario plantear diversos enfoques y situarse en el más conveniente, según el contexto del problema. En términos de la asignación de las distribuciones previas para \\(\\theta\\) y \\(\\sigma^2\\) es posible: Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son informativas. Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son no informativas. Suponer que la distribución previa para \\(\\theta\\) depende de \\(\\sigma^2\\) y escribirla como \\(p(\\theta \\mid \\sigma^2)\\), mientras que la distribución previa de \\(\\sigma^2\\) no depende de \\(\\theta\\) y se puede escribir como \\(p(\\sigma^2)\\). La distribución previa para el parámetro \\(\\theta\\) será \\[ \\begin{equation*} \\theta \\sim Normal(0,10000) \\end{equation*} \\] Y la distribución previa para el parámetro \\(\\sigma^2\\) será \\[ \\begin{equation*} \\sigma^2 \\sim IG(0.0001,0.0001) \\end{equation*} \\] La distribución posterior condicional de \\(\\theta\\) es \\[ \\begin{equation} \\theta \\mid \\sigma^2,\\mathbf{Y} \\sim Normal(\\mu_n,\\tau_n^2) \\end{equation} \\] En donde las expresiones para \\(\\mu_n\\) y \\(\\tau_n^2\\) están dados previamente. En el siguiente enlace enconará el libro: Modelos Bayesianos con R y STAN donde puede profundizar en el desarrollo matemático de los resultados anteriores. Obejtivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 4.4.1.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% filter(dam_ee == 1, ingcorte&gt;0) %&gt;% transmute(dam_ee, logIngreso = log(ingcorte +1)) Creando código de STAN data { int&lt;lower=0&gt; n; real y[n]; } parameters { real sigma; real theta; } transformed parameters { real sigma2; sigma2 = pow(sigma, 2); } model { y ~ normal(theta, sigma); theta ~ normal(0, 1000); sigma2 ~ inv_gamma(0.001, 0.001); } generated quantities { real ypred[n]; // vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,sigma); } } Preparando el código de STAN NormalMeanVar &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/5NormalMeanVar.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_NormalMedia &lt;- stan( file = NormalMeanVar, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia,&quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia2.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Normal/model_NormalMedia2.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\sigma^2\\) es: tabla_Nor2 &lt;- summary(model_NormalMedia, pars = c(&quot;theta&quot;, &quot;sigma2&quot;, &quot;sigma&quot;))$summary tabla_Nor2 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 12.4558 2e-04 0.0072 12.4422 12.4509 12.4556 12.4608 12.4700 1615.399 0.9991 sigma2 0.5066 2e-04 0.0068 0.4938 0.5017 0.5065 0.5112 0.5204 1907.921 0.9999 sigma 0.7118 1e-04 0.0047 0.7027 0.7083 0.7117 0.7150 0.7214 1906.898 0.9999 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) posterior_sigma2 &lt;- as.array(model_NormalMedia, pars = &quot;sigma2&quot;) (mcmc_dens_chains(posterior_sigma2) + mcmc_areas(posterior_sigma2) ) / mcmc_trace(posterior_sigma2) posterior_sigma &lt;- as.array(model_NormalMedia, pars = &quot;sigma&quot;) (mcmc_dens_chains(posterior_sigma) + mcmc_areas(posterior_sigma) ) / mcmc_trace(posterior_sigma) y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(exp(dataNormal$logIngreso)-1), y_pred2) + xlim(0,2000000) 4.4.2 Modelo de área: Multinomial En esta sección discutimos el modelamiento bayesiano de datos provenientes de una distribución multinomial que corresponde a una extensión multivariada de la distribución binomial. Suponga que \\(\\textbf{Y}=(Y_1,\\ldots,Y_K)^{T}\\) es un vector aleatorio con distribución multinomial, así, su distribución está parametrizada por el vector \\(\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_K)^{T}\\) y está dada por la siguiente expresión \\[ \\begin{equation} p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})=\\binom{n}{y_1,\\ldots,y_K}\\prod_{k=1}^K\\theta_k^{y_k} \\ \\ \\ \\ \\ \\theta_k&gt;0 \\texttt{ , } \\sum_{k=1}^{K}y_k=n \\texttt{ y } \\sum_{k=1}^K\\theta_k=1 \\end{equation} \\] Donde \\[ \\begin{equation*} \\binom{n}{y_1,\\ldots,y_K}=\\frac{n!}{y_1!\\cdots y_K!}. \\end{equation*} \\] Como cada parámetro \\(\\theta_k\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible asignar a la distribución de Dirichlet como la distribución previa del vector de parámetros. Por lo tanto la distribución previa del vector de parámetros \\(\\boldsymbol{\\theta}\\), parametrizada por el vector de hiperparámetros \\(\\boldsymbol{\\alpha}=(\\alpha_1,\\ldots,\\alpha_K)^{T}\\), está dada por \\[ \\begin{equation} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha})=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_K)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_K)} \\prod_{k=1}^K\\theta_k^{\\alpha_k-1} \\ \\ \\ \\ \\ \\alpha_k&gt;0 \\texttt{ y } \\sum_{k=1}^K\\theta_k=1 \\end{equation} \\] La distribución posterior del parámetro \\(\\boldsymbol{\\theta}\\) sigue una distribución \\(Dirichlet(y_1+\\alpha_1,\\ldots,y_K+\\alpha_K)\\) 4.4.2.1 Práctica en STAN Sea \\(Y\\) condición de actividad laboral dataMult &lt;- encuesta %&gt;% filter(condact3 %in% 1:3) %&gt;% transmute( empleo = as_factor(condact3)) %&gt;% group_by(empleo) %&gt;% tally() %&gt;% mutate(theta = n/sum(n)) tba(dataMult) empleo n theta Ocupado 92417 0.5291 Desocupado 8671 0.0496 Inactivo 73567 0.4212 donde 1 corresponde a Ocupado, 2 son los Desocupado y 3 son Inactivo Creando código de STAN data { int&lt;lower=0&gt; k; // Número de cátegoria int y[k]; // Número de exitos vector[k] alpha; // Parámetro de las distribción previa } parameters { simplex[k] theta; } transformed parameters { real delta; // Tasa de desocupación delta = theta[2]/ (theta[2] + theta[1]); // (Desocupado)/(Desocupado + Ocupado) } model { y ~ multinomial(theta); theta ~ dirichlet(alpha); } generated quantities { int ypred[k]; ypred = multinomial_rng(theta, sum(y)); } Preparando el código de STAN Multinom &lt;- &quot;Recursos/Día1/Sesion4/Data/modelosStan/6Multinom.stan&quot; Organizando datos para STAN sample_data &lt;- list(k = nrow(dataMult), y = dataMult$n, alpha = c(0.5, 0.5, 0.5)) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_Multinom &lt;- stan( file = Multinom, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_Multinom, &quot;Recursos/Día1/Sesion4/0Recursos/Multinomial/model_Multinom.rds&quot;) model_Multinom &lt;- readRDS(&quot;Recursos/Día1/Sesion4/0Recursos/Multinomial/model_Multinom.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\delta\\) es: tabla_Mul1 &lt;- summary(model_Multinom, pars = c(&quot;delta&quot;, &quot;theta&quot;))$summary tabla_Mul1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat delta 0.0857 0 0.0009 0.0840 0.0851 0.0858 0.0864 0.0875 1201.662 1.0032 theta[1] 0.5291 0 0.0012 0.5267 0.5283 0.5292 0.5300 0.5314 2032.595 1.0001 theta[2] 0.0496 0 0.0005 0.0486 0.0493 0.0496 0.0500 0.0506 1133.188 1.0038 theta[3] 0.4212 0 0.0012 0.4190 0.4205 0.4212 0.4220 0.4236 1874.742 1.0011 posterior_theta1 &lt;- as.array(model_Multinom, pars = &quot;theta[1]&quot;) (mcmc_dens_chains(posterior_theta1) + mcmc_areas(posterior_theta1) ) / mcmc_trace(posterior_theta1) posterior_theta2 &lt;- as.array(model_Multinom, pars = &quot;theta[2]&quot;) (mcmc_dens_chains(posterior_theta2) + mcmc_areas(posterior_theta2) ) / mcmc_trace(posterior_theta2) posterior_theta3 &lt;- as.array(model_Multinom, pars = &quot;theta[3]&quot;) (mcmc_dens_chains(posterior_theta3) + mcmc_areas(posterior_theta3) ) / mcmc_trace(posterior_theta3) posterior_delta &lt;- as.array(model_Multinom, pars = &quot;delta&quot;) (mcmc_dens_chains(posterior_delta) + mcmc_areas(posterior_delta) ) / mcmc_trace(posterior_delta) La imagen es muy pesada no se carga al repositorio. n &lt;- nrow(dataMult) y_pred_B &lt;- as.array(model_Multinom, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 50) y_pred2 &lt;- y_pred_B[, 1:n] ppc_dens_overlay(y = as.numeric(dataMult$n), y_pred2) "],["día-2---sesión-1--estimaciones-casen-y-función-generalizada-de-varianza.html", "Capítulo 5 Día 2 - Sesión 1- Estimaciones CASEN y Función Generalizada de Varianza", " Capítulo 5 Día 2 - Sesión 1- Estimaciones CASEN y Función Generalizada de Varianza Uno de los insumos más importantes en el modelo de áreas es la varianza del estimador directo, a nivel de dominio, la cual no puede calcularse de ningún modo. En correspondencia, este valor debe estimarse desde los datos recolectados en cada dominio. Sin embargo, en dominios en las que se cuenta con un tamaño de muestra muy pequeño, estas estimaciones no tendrán un buen comportamiento. Por ende, es muy útil utilizar un modelo de suavizamiento de las varianzas para eliminar el ruido y la volatilidad de estas estimaciones y extraer la verdadera señal del proceso Hidiroglou (2019) afirma que \\(E_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta}\\) y \\(V_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\sigma_{u}^2+\\tilde{\\sigma}^2_{d}\\), en donde el subíndice \\(\\mathscr{MP}\\) hace referencia a la inferencia doble que se debe tener en cuenta en este tipo de ajustes y define la medida de probabilidad conjunta entre el modelo y el diseño de muestreo. \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento y la inclusión de las covariables auxiliares (\\(\\boldsymbol{x}_{d}\\)). \\(\\mathscr{MP}\\) hace referencia a la medida de probabilidad inducida por el diseño de muestreo complejo que induce las estimaciones directas. La solución que acá se plantea se conoce con el nombre de Función Generalizada de Varianza, la cual consiste en ajustar un modelo log-lineal a la varianza directa estimada. Partiendo del hecho de que se tiene acceso a un estimador insesgado de \\(\\sigma^2\\), denotado por \\(\\hat{\\sigma}^2\\) se tiene que: \\[ E_{\\mathscr{MP}}\\left(\\hat{\\sigma}_{d}^{2}\\right)=E_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(\\hat{\\sigma}_{d}^{2}\\right)\\right)=E_{\\mathscr{M}}\\left(\\sigma_{d}^{2}\\right)=\\tilde{\\sigma}_{d}^{2} \\] La anterior igualdad puede interpretarse como que un estimador insesgado y simple de \\(\\tilde{\\sigma}_{d}^{2}\\) puede ser \\(\\hat{\\sigma}_{d}^{2}\\). Sin embargo, este estimador de muestreo es inestable cuando el tamaño de muestra es pequeño, que es justo el paradigma dominante en la estimación de áreas pequeñas. Rivest and Belmonte (2000) consideran modelos de suavizamiento para la estimación de las varianzas directas definidos de la siguiente manera: \\[ \\log\\left(\\hat{\\sigma}_{d}^{2}\\right)=\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}+\\boldsymbol{\\varepsilon}_{d} \\] En donde \\(\\boldsymbol{z}_{d}\\) es un vector de covariables explicativas que son funciones de \\(\\boldsymbol{x}_{d}\\), \\(\\boldsymbol{\\alpha}\\) es un vector de parámetros que deben ser estimados, \\(\\boldsymbol{\\varepsilon}_{d}\\) son errores aleatorios con media cero y varianza constante, que se asumen idénticamente distribuidos condicionalmente sobre \\(\\boldsymbol{z}_{d}\\). Del anterior modelo, la estimación suavizada de la varianza de muestreo está dada por: \\[ \\tilde{\\sigma}_{d}^{2}=E_{\\mathscr{MP}}\\left(\\sigma_{d}^{2}\\right)=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)\\times\\Delta \\] En donde, \\(E_{\\mathscr{MP}}\\left(\\varepsilon_{d}\\right)=\\Delta\\). No hay necesidad de especificar una distribución paramétrica para los errores de este modelo. Al utilizar el método de los momentos, se tiene el siguiente estimador insesgado para \\(\\Delta\\): \\[ \\hat{\\Delta}=\\frac{\\sum_{d=1}^{D}\\hat{\\sigma}_{d}^{2}}{\\sum_{d=1}^{D}\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)} \\] De la misma forma, al utilizar los procedimientos estándar en una regresión lineal, la estimación del coeficiente de parámetros de regresión está dada por la siguiente expresión: \\[ \\hat{\\boldsymbol{\\alpha}}=\\left(\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\boldsymbol{z}_{d}^{T}\\right)^{-1}\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\log\\left(\\hat{\\sigma}_{d}^{2}\\right) \\] Por último, el estimador suavizado de la varianza muestral está definido por: \\[ \\hat{\\tilde{\\sigma}}_{d}^{2}=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\hat{\\boldsymbol{\\alpha}}\\right)\\hat{\\Delta} \\] "],["datos-de-la-encuesta.html", "5.1 Datos de la encuesta", " 5.1 Datos de la encuesta El siguiente bloque de código utiliza varias librerías en R (tidyverse y magrittr), así como también utiliza una función definida en otro archivo (source(“Recursos/Día2/Sesion1/0Recursos/0Source_FH.R”)). Luego, el código carga la encuesta que esta almacenada en un archivo de datos en formato RDS y utiliza la función %&gt;% para encadenar una serie de transformaciones en los datos: transmute() se utiliza para seleccionar y renombrar columnas. Se crea una nueva variable llamada pobreza que se establece en 1 si la variable ingcorte(ingreso percapital) es menor que la variable lp, y en 0 en caso contrario. La función ifelse() se utiliza para asignar valores a la variable “pobreza” en función de si el ingreso de un individuo es menor o mayor que el umbral de pobreza. library(tidyverse) library(magrittr) source(&quot;Recursos/Día2/Sesion1/0Recursos/0Source_FH.R&quot;) encuesta &lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/encuesta2017CHL.Rds&quot;) %&gt;% transmute( dam = haven::as_factor(dam_ee ,levels = &quot;values&quot;), dam2 = haven::as_factor(comuna,levels = &quot;values&quot;), dam = str_pad(dam, width = 2, pad = &quot;0&quot;), dam2 = str_pad(dam2, width = 5, pad = &quot;0&quot;), wkx = `_fep`, upm = `_upm`, estrato = `_estrato`, pobreza = ifelse(ingcorte &lt; lp, 1 , 0)) dam: Corresponde al código asignado a la división administrativa mayor del país. dam2: Corresponde al código asignado a la segunda división administrativa del país. lp linea de pobreza definida por CEPAL. Factor de expansión por persona (wkx) dam dam2 wkx upm estrato pobreza 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 1 01 01101 39 1100100001 11001 1 01 01101 39 1100100001 11001 1 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 01 01101 39 1100100001 11001 0 En el siguiente bloque de código utiliza las librerías survey y srvyr para crear un diseño de muestreo a partir de una base de datos de encuestas. El diseño de muestreo incluye información sobre las unidades primarias de muestreo (UPM), los pesos de muestreo (wkx), y las estratas (estrato) utilizadas en el muestreo. Además, se utiliza la opción “survey.lonely.psu” para ajustar los tamaños de muestra en los grupos de unidades primarias de muestreo que no tienen otras unidades primarias de muestreo en el mismo grupo. library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- as_survey_design( ids = upm, weights = wkx, strata = estrato, nest = TRUE, .data = encuesta ) #summary(diseno) Para la estimación directa de la proporción se emplea la función direct.supr, disponible en el archivo 0Source_FH.R. Está función realiza las estimaciones y criterios de calidad en una encuesta de muestreo complejo con diseño estratificado y por conglomerados. Toma cinco argumentos: design.base, variable, group, upm y estrato. La función comienza cargando varios paquetes, como rlang, tidyverse, dplyr, survey y srvyr. Luego, los argumentos group, variable, upm y estrato se convierten en argumentos utilizando la función enquo. La función utiliza la encuesta de muestreo complejo design.base para calcular las estimaciones de los parámetros y los criterios de calidad. Utiliza la función survey_mean() de la librería survey para calcular la media y los intervalos de confianza de la variable de interés. La función también calcula otros indicadores de calidad, como el coeficiente de variación, el tamaño de muestra efectivo y el efecto del diseño. Luego, utiliza la función as.data.frame() para convertir los resultados en un objeto de marco de datos. Además, la función calcula otros criterios de calidad para determinar si las estimaciones son confiables. En particular, evalúa si se cumple un umbral mínimo para el número de grados de libertad, si la muestra es suficientemente grande y si el efecto del diseño es razonable. La función también tiene la opción de incluir o excluir ciertos grupos de muestreo basados en sus características. directodam2 &lt;- direct.supr(design.base = diseno, variable = pobreza, group = dam2, upm = upm, estrato = estrato) directodam2 %&gt;% group_by(Flag) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% tba() Flag n Incluir 102 Excluir 222 Para los dominios que no son excluidos se hace la transformación arcoseno, calculo del DEFF y varianza base_sae &lt;- directodam2 %&gt;% filter(Flag != &quot;Excluir&quot;) %&gt;% transmute( dam2 = dam2, # Id para los dominios nd = n, # Número de observaciones por dominios n_effec = n.eff, # n efectivo. pobreza = p, # Estimación de la variable pobreza_T = asin(sqrt(pobreza)), # Transformación arcoseno vardir = ee ^ 2, # Estimación de la varianza directa cv = CV, var_zd = 1 / (4 * n_effec), # Varianza para la tranformación arcsin deff_dam2 = deff # Deff por dominio ) # View(base_sae) tba(head(base_sae)) dam2 nd n_effec pobreza pobreza_T vardir cv var_zd deff_dam2 01101 6447 1540.5327 0.0764 0.2801 1e-04 13.7671 0.0002 4.1849 01107 3015 874.1965 0.1624 0.4148 4e-04 11.7319 0.0003 3.4489 02101 5473 729.6790 0.0905 0.3055 1e-04 13.0383 0.0003 7.5006 02201 1759 129.0530 0.0772 0.2816 5e-04 28.2624 0.0019 13.6301 03101 3757 717.2133 0.1011 0.3236 3e-04 16.0821 0.0003 5.2383 03301 1221 1165.6410 0.1106 0.3390 1e-04 10.4785 0.0002 1.0475 seguidamente se realiza la transformación \\(\\log(\\hat{\\sigma}^2_d)\\), además se realiza la selección de las columnas identificador del municipio (dam2), la estimación directa (pobreza), El número de personas en el dominio (nd) y la varianza estimada del para la estimación directa vardir,siendo esta la que transforma mediante la función log(). baseFGV &lt;- base_sae %&gt;% select(dam2, pobreza, nd, vardir) %&gt;% mutate(ln_sigma2 = log(vardir)) "],["análisis-gráfico.html", "5.2 Análisis gráfico", " 5.2 Análisis gráfico El primer gráfico, p1, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la variable pobreza, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como pobreza. El segundo gráfico, p2, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la variable nd, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Tamaño de muestra. El tercer gráfico, p3, muestra una gráfica de dispersión de la variable ln_sigma2 en función del producto de pobreza y nd, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Número de pobres. El cuarto gráfico, p4, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la raíz cuadrada de la variable pobreza, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Raiz cuadrada de pobreza. En general, los gráficos estan diseñados para explorar la relación entre ln_sigma2 y diferentes variables independientes, como pobreza, nd, y la raíz cuadrada de la pobreza. La elección de utilizar la función “loess” para suavizar las líneas en lugar de una línea recta puede ayudar a visualizar mejor las tendencias generales en los datos. theme_set(theme_bw()) # pobreza vs Ln_sigma2 # p1 &lt;- ggplot(baseFGV, aes(x = pobreza, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;pobreza&quot;) # Tamaño de muestra vs Ln_sigma2 # p2 &lt;- ggplot(baseFGV, aes(x = nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Tamaño de muestra&quot;) # Número de pobres vs Ln_sigma2 # p3 &lt;- ggplot(baseFGV, aes(x = pobreza * nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Número de pobres&quot;) # Raiz_pobreza vs Ln_sigma2 # p4 &lt;- ggplot(baseFGV, aes(x = sqrt(pobreza), y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Raiz cuadrada de pobreza&quot;) library(patchwork) (p1 | p2) / (p3 | p4) "],["modelo-para-la-varianza.html", "5.3 Modelo para la varianza", " 5.3 Modelo para la varianza El código ajusta un modelo de regresión lineal múltiple (utilizando la función lm()), donde ln_sigma2 es la variable respuesta y las variables predictoras son pobreza, nd, y varias transformaciones de éstas. El objetivo de este modelo es estimar la función generalizada de varianza (FGV) para los dominios observados. library(gtsummary) FGV1 &lt;- lm(ln_sigma2 ~ pobreza + I(nd^2) + I(sqrt(pobreza)), data = baseFGV) tbl_regression(FGV1) %&gt;% add_glance_table(include = c(r.squared, adj.r.squared)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #hrtyvzwnps .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #hrtyvzwnps .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hrtyvzwnps .gt_caption { padding-top: 4px; padding-bottom: 4px; } #hrtyvzwnps .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #hrtyvzwnps .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #hrtyvzwnps .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hrtyvzwnps .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hrtyvzwnps .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #hrtyvzwnps .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #hrtyvzwnps .gt_column_spanner_outer:first-child { padding-left: 0; } #hrtyvzwnps .gt_column_spanner_outer:last-child { padding-right: 0; } #hrtyvzwnps .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #hrtyvzwnps .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #hrtyvzwnps .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #hrtyvzwnps .gt_from_md > :first-child { margin-top: 0; } #hrtyvzwnps .gt_from_md > :last-child { margin-bottom: 0; } #hrtyvzwnps .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #hrtyvzwnps .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #hrtyvzwnps .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #hrtyvzwnps .gt_row_group_first td { border-top-width: 2px; } #hrtyvzwnps .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hrtyvzwnps .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #hrtyvzwnps .gt_first_summary_row.thick { border-top-width: 2px; } #hrtyvzwnps .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hrtyvzwnps .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hrtyvzwnps .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #hrtyvzwnps .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #hrtyvzwnps .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hrtyvzwnps .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hrtyvzwnps .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #hrtyvzwnps .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hrtyvzwnps .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hrtyvzwnps .gt_left { text-align: left; } #hrtyvzwnps .gt_center { text-align: center; } #hrtyvzwnps .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #hrtyvzwnps .gt_font_normal { font-weight: normal; } #hrtyvzwnps .gt_font_bold { font-weight: bold; } #hrtyvzwnps .gt_font_italic { font-style: italic; } #hrtyvzwnps .gt_super { font-size: 65%; } #hrtyvzwnps .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #hrtyvzwnps .gt_asterisk { font-size: 100%; vertical-align: 0; } #hrtyvzwnps .gt_indent_1 { text-indent: 5px; } #hrtyvzwnps .gt_indent_2 { text-indent: 10px; } #hrtyvzwnps .gt_indent_3 { text-indent: 15px; } #hrtyvzwnps .gt_indent_4 { text-indent: 20px; } #hrtyvzwnps .gt_indent_5 { text-indent: 25px; } Characteristic Beta 95% CI1 p-value pobreza -23 -36, -11 I(nd^2) 0.00 0.00, 0.00 I(sqrt(pobreza)) 23 16, 31 R² 0.612 Adjusted R² 0.600 1 CI = Confidence Interval Después de tener la estimación del modelo se debe obtener el valor de la constante \\(\\Delta\\) para lo cual se usa el siguiente código. delta.hat = sum(baseFGV$vardir) / sum(exp(fitted.values(FGV1))) De donde se obtiene que \\(\\Delta = 1.3509652\\). Final es posible obtener la varianza suavizada ejecutando el siguiente comando. hat.sigma &lt;- data.frame(dam2 = baseFGV$dam2, hat_var = delta.hat * exp(fitted.values(FGV1))) baseFGV &lt;- left_join(baseFGV, hat.sigma) tba(head(baseFGV, 10)) dam2 pobreza nd vardir ln_sigma2 hat_var 01101 0.0764 6447 0.0001 -9.1085 0.0001 01107 0.1624 3015 0.0004 -7.9210 0.0009 02101 0.0905 5473 0.0001 -8.8804 0.0002 02201 0.0772 1759 0.0005 -7.6500 0.0005 03101 0.1011 3757 0.0003 -8.2383 0.0004 03301 0.1106 1221 0.0001 -8.9152 0.0008 04101 0.1710 2719 0.0002 -8.4038 0.0011 04102 0.1930 2498 0.0005 -7.6308 0.0012 04203 0.1004 482 0.0011 -6.8172 0.0008 04301 0.0902 1323 0.0005 -7.6734 0.0006 Validación del modelo para la FGV par(mfrow = c(2, 2)) plot(FGV1) Comparación entre la varianza estimada versus la pronosticada por la FGV ggplot(baseFGV , aes(y = vardir, x = hat_var)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(x = &quot;FGV&quot;, y = &quot;VarDirEst&quot;) + ylab(&quot;Varianza del Estimador Directo&quot;) Predicción de la varianza suavizada base_sae &lt;- base_sae %&gt;% left_join(hat.sigma, by = &quot;dam2&quot;) El siguiente código utiliza la función mutate() del paquete dplyr para crear nuevas variables de la base de datos base_sae y luego guarda el resultado en un archivo RDS llamado base_FH_2018.rds. En concreto, el código realiza las siguientes operaciones: La variable deff_dam2 se ajusta a 1 cuando es NaN. La variable deff_FGV se calcula a partir de otras dos variables hat_var y vardir. Si vardir es 0, entonces deff_FGV se ajusta a 1. En caso contrario, se divide hat_var por vardir / deff_dam2 para obtener deff_FGV. La variable deff_FGV se regulariza utilizando el criterio MDS: si deff_FGV es menor que 1, se ajusta a 1. Finalmente, se calcula la variable n_eff_FGV dividiendo nd (el tamaño de la muestra) por deff_FGV. base_FH &lt;- base_sae %&gt;% mutate( deff_dam2 = ifelse(is.nan(deff_dam2), 1, deff_dam2), deff_FGV = ifelse( vardir == 0 , 1, hat_var / (vardir / deff_dam2) ), # Criterio MDS para regularizar el DeffFGV deff_FGV = ifelse(deff_FGV &lt; 1, 1, deff_FGV), n_eff_FGV = nd / deff_FGV ) saveRDS(object = base_FH, &quot;Recursos/Día2/Sesion1/Data/base_FH_2017.rds&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
